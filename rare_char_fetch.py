#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç”Ÿåƒ»å­—ä¸“é¡¹é‡‡é›† - é’ˆå¯¹æ€§å¢åŠ ç½•è§æ±‰å­—
ç­–ç•¥ï¼šé‡‡é›†ç”Ÿåƒ»å­—é›†ä¸­çš„ç‰¹å®šæ¡ç›®
"""

import requests
import json
import time
import re
from collections import Counter
import os

class RareCharFetcher:
    def __init__(self, existing_file='large_wikipedia_dataset.json'):
        self.base_url = "https://zh.wikipedia.org/w/api.php"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # åŠ è½½ç°æœ‰æ•°æ®
        print(f"ğŸ“‚ åŠ è½½ç°æœ‰æ•°æ®: {existing_file}")
        with open(existing_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.all_sentences = data['sentences']
        self.vocab_counter = Counter()
        
        # é‡æ–°ç»Ÿè®¡è¯æ±‡
        for sent in self.all_sentences:
            for char in sent:
                if '\u4e00' <= char <= '\u9fff':
                    self.vocab_counter[char] += 1
        
        print(f"   å·²æœ‰å¥å­: {len(self.all_sentences):,}")
        print(f"   å·²æœ‰è¯æ±‡: {len(self.vocab_counter):,}")
        print(f"   éœ€å¢åŠ : {max(0, 10000 - len(self.vocab_counter)):,}\n")
        
        # ç”Ÿåƒ»å­—é›†ä¸­çš„å…³é”®è¯
        self.rare_char_keywords = [
            # å§“æ°ï¼ˆç”Ÿåƒ»å§“ï¼‰
            'ä¸­å›½å§“æ°', 'å¤å§“', 'ç½•è§å§“æ°', 'ç™¾å®¶å§“',
            
            # åœ°åï¼ˆå«ç”Ÿåƒ»å­—ï¼‰
            'ä¸­å›½åœ°å', 'å¤ä»£åœ°å', 'éƒ¡å¿', 'å·åºœ', 'ä¹¡é•‡å',
            
            # åŠ¨æ¤ç‰©ï¼ˆå­¦åå«ç”Ÿåƒ»å­—ï¼‰
            'æ¤ç‰©å­¦å', 'åŠ¨ç‰©å­¦å', 'æ˜†è™«å', 'é¸Ÿç±»å', 'é±¼ç±»å',
            'è¯ç”¨æ¤ç‰©', 'ä¸­è¯æ', 'è‰æœ¬æ¤ç‰©', 'æœ¨æœ¬æ¤ç‰©',
            
            # çŸ¿ç‰©åŒ–çŸ³
            'çŸ¿ç‰©åç§°', 'å®çŸ³åç§°', 'åŒ–çŸ³åç§°', 'å²©çŸ³åç§°',
            
            # å¤ä»£æ–‡åŒ–
            'å¤ä»£å™¨ç‰©', 'é’é“œå™¨å', 'é™¶ç“·å™¨', 'ç‰å™¨åç§°',
            'å¤ä»£å®˜èŒ', 'ç§‘ä¸¾åˆ¶åº¦', 'å¤ä»£ç¤¼ä»ª',
            
            # ä¸­åŒ»è¯
            'ä¸­è¯æ–¹å‰‚', 'ç©´ä½åç§°', 'ç»ç»œåç§°', 'ç—…ç—‡åç§°',
            
            # å¤ç±å…¸æ•…
            'è¯—ç»', 'å°”é›…', 'è¯´æ–‡è§£å­—', 'åº·ç†™å­—å…¸',
            'å¤ä»£å…¸ç±', 'æ–‡çŒ®åç§°',
            
            # å®—æ•™æ–‡åŒ–
            'ä½›æ•™æœ¯è¯­', 'é“æ•™æœ¯è¯­', 'å¯ºåº™åç§°', 'é“è§‚åç§°',
            
            # å»ºç­‘åç§°
            'å¤ä»£å»ºç­‘', 'å®«æ®¿åç§°', 'åŸæ± åç§°', 'å›­æ—åç§°',
            
            # è‰ºæœ¯é—¨ç±»
            'ä¹¦æ³•å­—ä½“', 'ç¯†åˆ»å°ç« ', 'ç»˜ç”»æŠ€æ³•', 'æˆæ›²å‰§ç§',
            
            # å¤©æ–‡å†æ³•
            'æ˜Ÿå®¿åç§°', 'èŠ‚æ°”', 'å¹²æ”¯', 'å¤©æ–‡æœ¯è¯­',
            
            # ä¸“ä¸šæœ¯è¯­
            'çººç»‡å“å', 'å·¥è‰ºåç§°', 'ä¹å™¨åç§°', 'å…µå™¨åç§°'
        ]
    
    def get_specific_articles(self, keyword, count=50):
        """è·å–ç‰¹å®šå…³é”®è¯çš„æ–‡ç« """
        print(f"ğŸ” {keyword}")
        
        articles = []
        params = {
            'action': 'query',
            'format': 'json',
            'list': 'search',
            'srsearch': keyword,
            'srnamespace': 0,
            'srlimit': count
        }
        
        try:
            response = self.session.get(self.base_url, params=params, timeout=10)
            data = response.json()
            
            for item in data.get('query', {}).get('search', []):
                articles.append(item['title'])
            
            print(f"   æ‰¾åˆ° {len(articles)} ç¯‡")
        except Exception as e:
            print(f"   å‡ºé”™: {e}")
        
        return articles
    
    def fetch_and_extract_chars(self, title):
        """è·å–æ–‡ç« å¹¶æå–æ–°æ±‰å­—"""
        params = {
            'action': 'query',
            'format': 'json',
            'titles': title,
            'prop': 'extracts',
            'explaintext': True
        }
        
        try:
            response = self.session.get(self.base_url, params=params, timeout=10)
            data = response.json()
            
            pages = data.get('query', {}).get('pages', {})
            for page_id, page_data in pages.items():
                if 'extract' in page_data:
                    text = page_data['extract']
                    
                    # æå–æ–°æ±‰å­—
                    new_chars = set()
                    for char in text:
                        if '\u4e00' <= char <= '\u9fff':
                            if char not in self.vocab_counter:
                                new_chars.add(char)
                            self.vocab_counter[char] += 1
                    
                    # æå–å¥å­
                    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)
                    for sent in sentences:
                        sent = sent.strip()
                        if 10 <= len(sent) <= 200 and re.search(r'[\u4e00-\u9fff]', sent):
                            self.all_sentences.append(sent)
                    
                    return len(new_chars)
        except:
            pass
        
        return 0
    
    def process_category(self, keyword):
        """å¤„ç†ä¸€ä¸ªç±»åˆ«"""
        articles = self.get_specific_articles(keyword, count=50)
        
        if not articles:
            return 0
        
        new_char_count = 0
        for title in articles:
            new_chars = self.fetch_and_extract_chars(title)
            new_char_count += new_chars
        
        print(f"   +{new_char_count}æ–°å­— â†’ æ€»è¯æ±‡: {len(self.vocab_counter):,}\n")
        return new_char_count
    
    def save_dataset(self, filename='large_wikipedia_dataset.json'):
        """ä¿å­˜æ•°æ®é›†"""
        print(f"\nğŸ’¾ ä¿å­˜æ•°æ®é›†...")
        
        # å»é‡å¥å­
        self.all_sentences = list(set(self.all_sentences))
        
        # æ„å»ºè¯æ±‡è¡¨ï¼ˆæ‰€æœ‰å‡ºç°è¿‡çš„å­—ï¼‰
        vocab_list = list(self.vocab_counter.keys())
        
        dataset = {
            'sentences': self.all_sentences,
            'vocab': vocab_list,
            'metadata': {
                'sentence_count': len(self.all_sentences),
                'vocab_size': len(vocab_list),
                'avg_sentence_length': sum(len(s) for s in self.all_sentences) / len(self.all_sentences),
                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'min_freq': 1
            }
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ“ å·²ä¿å­˜\n")
        return dataset

def main():
    print('='*80)
    print('ç”Ÿåƒ»å­—ä¸“é¡¹é‡‡é›†')
    print('='*80 + '\n')
    
    fetcher = RareCharFetcher()
    
    target = 10000
    current = len(fetcher.vocab_counter)
    
    if current >= target:
        print(f"âœ… å·²è¾¾æ ‡ï¼({current:,} >= {target:,})")
        return
    
    print(f"ğŸ“Œ å½“å‰: {current:,} | ç›®æ ‡: {target:,} | è¿˜éœ€: {target - current:,}\n")
    print('='*80 + '\n')
    
    for idx, keyword in enumerate(fetcher.rare_char_keywords, 1):
        current = len(fetcher.vocab_counter)
        remaining = target - current
        
        print(f"[{idx}/{len(fetcher.rare_char_keywords)}] è¿˜éœ€{remaining}å­—")
        
        fetcher.process_category(keyword)
        
        current = len(fetcher.vocab_counter)
        if current >= target:
            print(f"\nğŸ‰ è¯æ±‡é‡è¾¾æ ‡ï¼({current:,} >= {target:,})\n")
            break
        
        time.sleep(1)
    
    # ä¿å­˜
    print('='*80)
    dataset = fetcher.save_dataset()
    
    print('='*80)
    print('æœ€ç»ˆç»Ÿè®¡')
    print('='*80)
    print(f"âœ… å¥å­æ€»æ•°:   {dataset['metadata']['sentence_count']:,}")
    print(f"âœ… è¯æ±‡é‡:     {dataset['metadata']['vocab_size']:,}")
    print('='*80 + '\n')
    
    if dataset['metadata']['vocab_size'] >= 10000:
        print("ğŸ‰ğŸ‰ğŸ‰ æ­å–œï¼è¯æ±‡é‡è¾¾æ ‡ï¼å¯ä»¥å¼€å§‹è®­ç»ƒï¼")
    else:
        print(f"âš ï¸  è¿˜å·® {10000 - dataset['metadata']['vocab_size']:,} ä¸ªè¯æ±‡")

if __name__ == '__main__':
    main()
