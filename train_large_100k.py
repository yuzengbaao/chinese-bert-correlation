#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¤§è§„æ¨¡è®­ç»ƒè„šæœ¬ - 100Kæ­¥è®­ç»ƒ
ç›®æ ‡: åœ¨100K+å¥å­æ•°æ®é›†ä¸Šè®­ç»ƒï¼Œè¾¾åˆ°Pearson r > 0.85
ä¼˜åŒ–: æ›´å¤§æ‰¹é‡, æ›´å¤šepoch, æ¢¯åº¦ç´¯ç§¯
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import json
import logging
import os
import random
import numpy as np
from datetime import datetime

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_100k_internal.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ä¿®å¤PyTorchéšæœºç§å­
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

set_seed(42)

# ==================== æ¨¡å‹å®šä¹‰ ====================

class BERTModel(nn.Module):
    """BERTæ¨¡å‹ - ä¸ä¹‹å‰ä¿æŒä¸€è‡´"""
    def __init__(self, vocab_size, hidden_size=512, num_layers=12, num_heads=8, ff_size=2048, dropout=0.1):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, hidden_size)
        self.position_embedding = nn.Embedding(512, hidden_size)
        self.token_type_embedding = nn.Embedding(2, hidden_size)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=ff_size,
            dropout=dropout,
            activation='gelu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        
        self.mlm_head = nn.Linear(hidden_size, vocab_size)
        self.nsp_head = nn.Linear(hidden_size, 2)
        
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, input_ids, token_type_ids, attention_mask=None):
        seq_len = input_ids.size(1)
        position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0).expand_as(input_ids)
        
        embeddings = self.embedding(input_ids) + self.position_embedding(position_ids) + self.token_type_embedding(token_type_ids)
        embeddings = self.dropout(embeddings)
        
        if attention_mask is not None:
            attention_mask = attention_mask.float()
            attention_mask = (1.0 - attention_mask) * -10000.0
        
        hidden_states = self.transformer(embeddings, src_key_padding_mask=attention_mask)
        
        mlm_logits = self.mlm_head(hidden_states)
        cls_output = hidden_states[:, 0, :]
        nsp_logits = self.nsp_head(cls_output)
        
        return mlm_logits, nsp_logits

# ==================== æ•°æ®é›† ====================

class LargeBERTDataset(Dataset):
    """å¤§è§„æ¨¡BERTæ•°æ®é›†"""
    def __init__(self, sentences, vocab, max_len=128):
        self.sentences = sentences
        self.vocab = vocab
        self.max_len = max_len
        
        # æ„å»ºå­—ç¬¦æ˜ å°„
        self.char2idx = {char: idx + 4 for idx, char in enumerate(vocab)}
        self.char2idx.update({
            '[PAD]': 0,
            '[UNK]': 1,
            '[CLS]': 2,
            '[SEP]': 3,
            '[MASK]': len(vocab) + 4
        })
        self.idx2char = {idx: char for char, idx in self.char2idx.items()}
        
        logger.info(f"Dataset initialized: {len(sentences):,} sentences, vocab size: {len(self.char2idx):,}")
    
    def __len__(self):
        return len(self.sentences)
    
    def __getitem__(self, idx):
        # è·å–ä¸¤ä¸ªå¥å­ï¼ˆ50%ç›¸é‚»ï¼Œ50%éšæœºï¼‰
        sent1 = self.sentences[idx]
        
        if random.random() > 0.5 and idx + 1 < len(self.sentences):
            sent2 = self.sentences[idx + 1]
            nsp_label = 0  # IsNext
        else:
            sent2 = self.sentences[random.randint(0, len(self.sentences) - 1)]
            nsp_label = 1  # NotNext
        
        # ç¼–ç 
        tokens1 = [self.char2idx.get(c, self.char2idx['[UNK]']) for c in sent1]
        tokens2 = [self.char2idx.get(c, self.char2idx['[UNK]']) for c in sent2]
        
        # æˆªæ–­
        max_len_per_sent = (self.max_len - 3) // 2
        tokens1 = tokens1[:max_len_per_sent]
        tokens2 = tokens2[:max_len_per_sent]
        
        # æ‹¼æ¥: [CLS] sent1 [SEP] sent2 [SEP]
        tokens = [self.char2idx['[CLS]']] + tokens1 + [self.char2idx['[SEP]']] + tokens2 + [self.char2idx['[SEP]']]
        token_type_ids = [0] * (len(tokens1) + 2) + [1] * (len(tokens2) + 1)
        
        # Padding
        padding_len = self.max_len - len(tokens)
        tokens += [self.char2idx['[PAD]']] * padding_len
        token_type_ids += [0] * padding_len
        
        return {
            'input_ids': torch.tensor(tokens[:self.max_len], dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids[:self.max_len], dtype=torch.long),
            'nsp_label': torch.tensor(nsp_label, dtype=torch.long)
        }

def create_mlm_task(batch, mask_prob=0.15, device='cuda'):
    """åˆ›å»ºMLMä»»åŠ¡"""
    input_ids = batch['input_ids'].clone().to(device)
    labels = input_ids.clone()
    
    # éšæœºmask
    prob_matrix = torch.rand(input_ids.shape, device=device)
    mask_arr = (prob_matrix < mask_prob).float()
    
    # ä¸maskç‰¹æ®Štoken
    special_tokens = [0, 2, 3]  # [PAD], [CLS], [SEP]
    for token_id in special_tokens:
        mask_arr = mask_arr * (input_ids != token_id).float()
    
    # åº”ç”¨mask
    mask_idx = mask_arr.bool()
    labels[~mask_idx] = -100
    
    # 80% mask, 10% random, 10% keep
    mask_token_id = input_ids.max().item() + 1
    random_tokens = torch.randint(4, input_ids.max().item(), input_ids.shape, device=device)
    
    prob_replace = torch.rand(input_ids.shape, device=device)
    input_ids[mask_idx] = torch.where(
        prob_replace[mask_idx] < 0.8,
        torch.tensor(mask_token_id, device=device),
        torch.where(
            prob_replace[mask_idx] < 0.9,
            random_tokens[mask_idx],
            input_ids[mask_idx]
        )
    )
    
    return input_ids, labels

# ==================== è®­ç»ƒå™¨ ====================

class Trainer:
    """è®­ç»ƒå™¨ - æ”¯æŒæ¢¯åº¦ç´¯ç§¯"""
    def __init__(self, model, train_loader, device='cuda', lr=5e-5, total_steps=100000, 
                 warmup_steps=2000, accumulation_steps=4, checkpoint_dir='checkpoints_100k'):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.device = device
        self.total_steps = total_steps
        self.accumulation_steps = accumulation_steps
        self.checkpoint_dir = checkpoint_dir
        
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = self.get_cosine_schedule_with_warmup(
            self.optimizer, warmup_steps, total_steps
        )
        
        # æŸå¤±å‡½æ•°
        self.mlm_criterion = nn.CrossEntropyLoss(ignore_index=-100)
        self.nsp_criterion = nn.CrossEntropyLoss()
        
        # ç»Ÿè®¡
        self.history = {
            'steps': [],
            'mlm_acc': [],
            'nsp_acc': [],
            'loss': [],
            'lr': []
        }
        
        logger.info(f"Trainer initialized:")
        logger.info(f"  Total steps: {total_steps:,}")
        logger.info(f"  Warmup steps: {warmup_steps:,}")
        logger.info(f"  Gradient accumulation: {accumulation_steps}")
        logger.info(f"  Effective batch size: {train_loader.batch_size * accumulation_steps}")
    
    @staticmethod
    def get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps):
        """ä½™å¼¦é€€ç«å­¦ä¹ ç‡"""
        def lr_lambda(current_step):
            if current_step < warmup_steps:
                return float(current_step) / float(max(1, warmup_steps))
            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))
            return max(0.0, 0.5 * (1.0 + np.cos(np.pi * progress)))
        
        return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    def train(self, start_step=0, log_interval=100, save_interval=1000):
        """è®­ç»ƒä¸»å¾ªç¯"""
        self.model.train()
        global_step = start_step
        self.optimizer.zero_grad()
        
        logger.info(f"\n{'='*80}")
        logger.info("Starting training...")
        logger.info(f"{'='*80}\n")
        
        epoch = 0
        while global_step < self.total_steps:
            epoch += 1
            logger.info(f"Epoch {epoch}")
            
            for batch_idx, batch in enumerate(self.train_loader):
                # MLMä»»åŠ¡
                input_ids, mlm_labels = create_mlm_task(batch, device=self.device)
                token_type_ids = batch['token_type_ids'].to(self.device)
                nsp_labels = batch['nsp_label'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                mlm_logits, nsp_logits = self.model(input_ids, token_type_ids)
                
                # è®¡ç®—æŸå¤±
                mlm_loss = self.mlm_criterion(
                    mlm_logits.view(-1, mlm_logits.size(-1)),
                    mlm_labels.view(-1)
                )
                nsp_loss = self.nsp_criterion(nsp_logits, nsp_labels)
                loss = mlm_loss + nsp_loss
                
                # åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ç´¯ç§¯ï¼‰
                loss = loss / self.accumulation_steps
                loss.backward()
                
                # æ›´æ–°å‚æ•°
                if (batch_idx + 1) % self.accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()
                    
                    global_step += 1
                    
                    # è®°å½•æŒ‡æ ‡
                    if global_step % log_interval == 0:
                        mlm_acc = self.calculate_mlm_accuracy(mlm_logits, mlm_labels)
                        nsp_acc = (nsp_logits.argmax(dim=1) == nsp_labels).float().mean().item()
                        
                        self.history['steps'].append(global_step)
                        self.history['mlm_acc'].append(mlm_acc)
                        self.history['nsp_acc'].append(nsp_acc)
                        self.history['loss'].append(loss.item() * self.accumulation_steps)
                        self.history['lr'].append(self.scheduler.get_last_lr()[0])
                        
                        logger.info(
                            f"Step: {global_step:,} | "
                            f"MLM Acc: {mlm_acc:.4f} | "
                            f"NSP Acc: {nsp_acc:.4f} | "
                            f"Loss: {loss.item() * self.accumulation_steps:.4f} | "
                            f"LR: {self.scheduler.get_last_lr()[0]:.2e}"
                        )
                    
                    # ä¿å­˜æ£€æŸ¥ç‚¹
                    if global_step % save_interval == 0:
                        self.save_checkpoint(global_step)
                    
                    # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                    if global_step >= self.total_steps:
                        break
            
            if global_step >= self.total_steps:
                break
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.save_final_model()
        logger.info(f"\nâœ… Training completed! Total steps: {global_step:,}\n")
    
    @staticmethod
    def calculate_mlm_accuracy(logits, labels):
        """è®¡ç®—MLMå‡†ç¡®åº¦"""
        preds = logits.argmax(dim=-1)
        mask = labels != -100
        if mask.sum() == 0:
            return 0.0
        correct = (preds == labels) & mask
        return (correct.sum() / mask.sum()).item()
    
    def save_checkpoint(self, step):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = os.path.join(self.checkpoint_dir, f'step_{step}_checkpoint.pth')
        torch.save({
            'step': step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'history': self.history
        }, checkpoint_path)
        logger.info(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")
    
    def save_final_model(self):
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹"""
        model_path = 'stage4_large_100k_final.pth'
        torch.save(self.model.state_dict(), model_path)
        logger.info(f"ğŸ’¾ Final model saved: {model_path}")
        
        # ä¿å­˜è®­ç»ƒå†å²
        history_path = 'training_history_100k.json'
        with open(history_path, 'w') as f:
            json.dump(self.history, f, indent=2)
        logger.info(f"ğŸ’¾ Training history saved: {history_path}")

# ==================== ä¸»å‡½æ•° ====================

def main():
    """ä¸»å‡½æ•°"""
    logger.info('\n' + '='*80)
    logger.info('å¤§è§„æ¨¡BERTè®­ç»ƒ - 100Kæ­¥å®éªŒ')
    logger.info('='*80 + '\n')
    
    # 1. åŠ è½½æ•°æ®
    logger.info("Loading dataset...")
    with open('large_wikipedia_dataset.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    sentences = data['sentences']
    vocab = data['vocab']
    
    logger.info(f"âœ… Dataset loaded:")
    logger.info(f"   Sentences: {len(sentences):,}")
    logger.info(f"   Vocab size: {len(vocab):,}\n")
    
    # 2. åˆ›å»ºæ•°æ®é›†å’ŒåŠ è½½å™¨
    dataset = LargeBERTDataset(sentences, vocab, max_len=128)
    
    # æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼ˆä»16å¢è‡³32ï¼‰
    train_loader = DataLoader(
        dataset,
        batch_size=32,  # å¢å¤§æ‰¹é‡
        shuffle=True,
        num_workers=0,
        drop_last=True
    )
    
    logger.info(f"âœ… DataLoader created:")
    logger.info(f"   Batch size: 32")
    logger.info(f"   Gradient accumulation: 4")
    logger.info(f"   Effective batch size: 128\n")
    
    # 3. åˆ›å»ºæ¨¡å‹
    vocab_size = len(dataset.char2idx)
    model = BERTModel(
        vocab_size=vocab_size,
        hidden_size=512,
        num_layers=12,
        num_heads=8,
        ff_size=2048,
        dropout=0.1
    )
    
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"âœ… Model created:")
    logger.info(f"   Parameters: {total_params/1e6:.2f}M\n")
    
    # 4. åˆ›å»ºè®­ç»ƒå™¨
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"Device: {device}\n")
    
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        device=device,
        lr=5e-5,
        total_steps=100000,
        warmup_steps=2000,
        accumulation_steps=4,
        checkpoint_dir='checkpoints_100k'
    )
    
    # 5. å¼€å§‹è®­ç»ƒ
    trainer.train(
        start_step=0,
        log_interval=100,
        save_interval=2000  # æ¯2000æ­¥ä¿å­˜ä¸€æ¬¡
    )
    
    logger.info("\nğŸ‰ All done!\n")

if __name__ == '__main__':
    main()
