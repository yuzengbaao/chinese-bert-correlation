# Dataset Documentation | æ•°æ®é›†è¯´æ˜

## ğŸ“š Overview | æ¦‚è§ˆ

The Chinese BERT Correlation Study dataset consists of **325,537 Chinese sentences** collected from Chinese Wikipedia, covering **59 specialized domains** with a vocabulary of **10,049 unique characters**.

æœ¬æ•°æ®é›†åŒ…å«ä»ä¸­æ–‡ç»´åŸºç™¾ç§‘é‡‡é›†çš„ **325,537ä¸ªä¸­æ–‡å¥å­**ï¼Œæ¶µç›– **59ä¸ªä¸“ä¸šé¢†åŸŸ**ï¼Œè¯æ±‡é‡è¾¾ **10,049ä¸ªæ±‰å­—**ã€‚

---

## ğŸ“Š Dataset Statistics | æ•°æ®é›†ç»Ÿè®¡

| Metric | Value | Description |
|--------|-------|-------------|
| **Total Sentences** | 325,537 | æ€»å¥å­æ•° |
| **Unique Characters** | 10,049 | ç‹¬ç‰¹æ±‰å­—æ•° |
| **Average Sentence Length** | 46.2 chars | å¹³å‡å¥é•¿ |
| **Duplication Rate** | 0.00% | å»é‡ç‡ |
| **Specialized Domains** | 59 | ä¸“ä¸šé¢†åŸŸæ•° |
| **Source** | Chinese Wikipedia | æ•°æ®æ¥æº |
| **File Size** | ~44.5 MB | æ–‡ä»¶å¤§å° |
| **Format** | JSON | æ–‡ä»¶æ ¼å¼ |

---

## ğŸ·ï¸ Domain Coverage | é¢†åŸŸè¦†ç›–

The dataset covers 59 specialized domains to ensure vocabulary diversity:

### 1. **People & Society | äººç‰©ä¸ç¤¾ä¼š**
- ä¸­å›½å§“æ° (Chinese surnames)
- å†å²äººç‰© (Historical figures)
- ç°ä»£åäºº (Modern celebrities)

### 2. **Geography | åœ°ç†**
- éƒ¡å¿ (Counties and prefectures)
- åŸå¸‚ (Cities)
- å±±å·æ²³æµ (Mountains and rivers)
- åèƒœå¤è¿¹ (Scenic spots)

### 3. **Biology | ç”Ÿç‰©**
- æ˜†è™«å (Insect names) - 86+ rare characters
- é±¼ç±»å (Fish names) - 68+ rare characters
- é¸Ÿç±» (Birds)
- æ¤ç‰© (Plants)
- ä¸­è¯æ (Traditional Chinese medicine) - 68+ rare characters

### 4. **Culture & History | æ–‡åŒ–å†å²**
- å¤ä»£å™¨ç‰© (Ancient artifacts)
- é’é“œå™¨å (Bronze ware) - 52+ rare characters
- ä¹¦æ³•ä½œå“ (Calligraphy works)
- æ–‡å­¦ä½œå“ (Literary works)
- æˆè¯­å…¸æ•… (Idioms and allusions)

### 5. **Science & Technology | ç§‘æŠ€**
- å¤©æ–‡å­¦ (Astronomy)
- ç‰©ç†å­¦ (Physics)
- åŒ–å­¦ (Chemistry)
- æ•°å­¦ (Mathematics)
- è®¡ç®—æœºç§‘å­¦ (Computer science)

### 6. **Arts & Entertainment | è‰ºæœ¯å¨±ä¹**
- éŸ³ä¹ (Music)
- ç»˜ç”» (Painting)
- ç”µå½± (Movies)
- æˆæ›² (Traditional opera)

### 7. **Other Domains | å…¶ä»–é¢†åŸŸ**
- é¥®é£Ÿæ–‡åŒ– (Culinary culture)
- å»ºç­‘ (Architecture)
- èŠ‚æ—¥ä¹ ä¿— (Festivals and customs)
- å®—æ•™å“²å­¦ (Religion and philosophy)
- And 30+ more...

---

## ğŸ”§ Data Collection Process | æ•°æ®é‡‡é›†è¿‡ç¨‹

### Strategy | ç­–ç•¥

The dataset was collected using a **multi-stage rare character collection strategy**:

```
Stage 1: General Collection (27K sentences, 4.7K vocab)
   â†“
Stage 2: Professional Domain Expansion (104K sentences, 6.4K vocab)
   â†“
Stage 3: Enhanced Vocabulary Expansion (150K sentences, 9.2K vocab)
   â†“
Stage 4: Rare Character Targeted Collection (325K sentences, 10K vocab) âœ…
```

### Script | è„šæœ¬

The main data collection script is `rare_char_fetch.py`, which:

1. **Targets rare characters** from 59 specialized domains
2. **Fetches full articles** from Chinese Wikipedia
3. **Extracts quality sentences** (length 30-100 characters)
4. **Removes duplicates** to ensure uniqueness
5. **Validates character coverage** to reach 10K vocabulary goal

### Key Breakthroughs | å…³é”®çªç ´

- **Category "éƒ¡å¿" (Counties)**: +109 new characters
- **Category "ä¸­å›½å§“æ°" (Surnames)**: +92 new characters
- **Category "æ˜†è™«å" (Insects)**: +86 new characters
- **Category "é±¼ç±»å" (Fish)**: +68 new characters
- **Category "ä¸­è¯æ" (TCM)**: +68 new characters
- **Category "é’é“œå™¨å" (Bronzeware)**: +52 new characters (final push to 10,049!)

---

## ğŸ“ Data Format | æ•°æ®æ ¼å¼

### File Structure

```json
{
  "sentences": [
    "ä¸­å›½æ˜¯ä¸–ç•Œä¸Šå†å²æœ€æ‚ ä¹…çš„æ–‡æ˜å¤å›½ä¹‹ä¸€ã€‚",
    "åŒ—äº¬æ˜¯ä¸­åäººæ°‘å…±å’Œå›½çš„é¦–éƒ½ã€‚",
    ...
  ]
}
```

### Fields

- **sentences** (array): List of Chinese sentences
  - Each sentence is a string
  - Length range: 30-100 characters
  - No duplicates
  - UTF-8 encoding

---

## âœ… Quality Assurance | è´¨é‡ä¿è¯

### Validation Checks

The dataset passes all quality checks via `verify_dataset.py`:

```python
âœ… Sentence count: 325,537 / 100,000 (target)
âœ… Vocabulary: 10,049 / 10,000 (target)
âœ… Average sentence length: 46.2 (within 30-100 range)
âœ… Duplication rate: 0.00% (< 5% threshold)
âœ… Data completeness: 100%
```

### Filtering Criteria

Sentences are included only if they:
1. Contain 30-100 characters
2. Are not duplicates
3. Come from verified Wikipedia articles
4. Pass UTF-8 encoding validation

---

## ğŸš€ Usage Examples | ä½¿ç”¨ç¤ºä¾‹

### Loading the Dataset

```python
import json

# Load dataset
with open('large_wikipedia_dataset.json', 'r', encoding='utf-8') as f:
    data = json.load(f)

sentences = data['sentences']
print(f"Total sentences: {len(sentences)}")
print(f"Sample: {sentences[0]}")
```

### Building Vocabulary

```python
from collections import Counter

# Count character frequency
all_chars = ''.join(sentences)
char_freq = Counter(all_chars)

print(f"Unique characters: {len(char_freq)}")
print(f"Top 10 characters: {char_freq.most_common(10)}")
```

### Preparing Training Data

```python
# For BERT MLM training
def prepare_mlm_data(sentences, mask_prob=0.15):
    """Prepare masked language modeling data"""
    masked_sentences = []
    for sent in sentences:
        # Implement masking logic
        # ...
    return masked_sentences
```

---

## ğŸ“Š Comparison with Other Datasets | ä¸å…¶ä»–æ•°æ®é›†å¯¹æ¯”

| Dataset | Sentences | Vocabulary | Domains | Duplication | Source |
|---------|-----------|-----------|---------|-------------|--------|
| **Ours** | **325,537** | **10,049** | **59** | **0.00%** | Wikipedia |
| Previous (50K) | 27,368 | 4,728 | 20 | 0.02% | Wikipedia |
| Common Crawl | Millions | ~8,000 | Mixed | High | Web |
| News Corpus | ~100K | ~5,000 | News | Medium | News sites |

**Advantages | ä¼˜åŠ¿:**
- âœ… Large vocabulary (10K+ characters)
- âœ… High quality (zero duplication)
- âœ… Diverse domains (59 specialized areas)
- âœ… Verified source (Wikipedia)
- âœ… Balanced rare character coverage

---

## ğŸ”¬ Research Applications | ç ”ç©¶åº”ç”¨

This dataset is particularly useful for:

1. **Chinese NLP Model Training**
   - Pre-training BERT models
   - Language model fine-tuning
   - Transfer learning experiments

2. **Rare Character Handling**
   - Testing model performance on rare characters
   - Vocabulary coverage analysis
   - Character-level modeling

3. **Domain-Specific Studies**
   - Cross-domain generalization
   - Domain adaptation
   - Specialized vocabulary learning

4. **Correlation Studies**
   - Training dynamics analysis
   - Performance prediction
   - Optimal training step calculation

---

## ğŸ“ Citation | å¼•ç”¨

If you use this dataset in your research:

```bibtex
@misc{chinese_wikipedia_dataset_2025,
  title={Chinese Wikipedia Dataset for BERT Training},
  author={Yuzengbaao},
  year={2025},
  howpublished={Chinese BERT Correlation Study},
  note={325,537 sentences, 10,049 vocabulary, 59 domains}
}
```

---

## ğŸ“® Contact | è”ç³»æ–¹å¼

For dataset-related questions:
- GitHub Issues: https://github.com/yuzengbaao/chinese-bert-correlation/issues
- Email: yuzengbaao@gmail.com

---

## ğŸ“œ License | è®¸å¯è¯

The dataset is released under MIT License. Chinese Wikipedia content is under CC BY-SA 3.0.

æ•°æ®é›†é‡‡ç”¨MITè®¸å¯è¯ã€‚ä¸­æ–‡ç»´åŸºç™¾ç§‘å†…å®¹éµå¾ª CC BY-SA 3.0 è®¸å¯ã€‚
