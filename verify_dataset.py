#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
éªŒè¯large_wikipedia_dataset.jsonçš„è´¨é‡
"""

import json
import sys

def verify_dataset(file_path='large_wikipedia_dataset.json'):
    """éªŒè¯æ•°æ®é›†è´¨é‡"""
    
    print("=" * 80)
    print("æ•°æ®é›†è´¨é‡éªŒè¯")
    print("=" * 80)
    
    try:
        # åŠ è½½æ•°æ®
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        sentences = data.get('sentences', [])
        vocab = data.get('vocab', [])
        metadata = data.get('metadata', {})
        
        # åŸºç¡€ç»Ÿè®¡
        print("\nã€åŸºç¡€ç»Ÿè®¡ã€‘")
        print(f"âœ“ å¥å­æ€»æ•°:       {len(sentences):,}")
        print(f"âœ“ è¯æ±‡è¡¨å¤§å°:     {len(vocab):,}")
        print(f"âœ“ å¹³å‡å¥é•¿:       {metadata.get('avg_sentence_length', 0):.1f} å­—ç¬¦")
        print(f"âœ“ å»é‡ç‡:         {metadata.get('deduplication_rate', 0):.2f}%")
        
        # è´¨é‡æ£€æŸ¥
        print("\nã€è´¨é‡æ£€æŸ¥ã€‘")
        
        # 1. æ•°é‡è¾¾æ ‡
        sentence_ok = len(sentences) >= 100000
        vocab_ok = len(vocab) >= 10000
        
        print(f"{'âœ…' if sentence_ok else 'âŒ'} å¥å­æ•°é‡: {len(sentences):,} / 100,000 (ç›®æ ‡)")
        print(f"{'âœ…' if vocab_ok else 'âŒ'} è¯æ±‡é‡:   {len(vocab):,} / 10,000 (ç›®æ ‡)")
        
        # 2. å¥é•¿åˆ†å¸ƒ
        lengths = [len(s) for s in sentences[:1000]]  # é‡‡æ ·å‰1000å¥
        avg_len = sum(lengths) / len(lengths)
        min_len = min(lengths)
        max_len = max(lengths)
        
        length_ok = 30 <= avg_len <= 100
        print(f"{'âœ…' if length_ok else 'âš ï¸ '} å¹³å‡å¥é•¿: {avg_len:.1f} (å»ºè®®: 30-100)")
        print(f"  - æœ€çŸ­: {min_len}")
        print(f"  - æœ€é•¿: {max_len}")
        
        # 3. å»é‡ç‡
        dedup_rate = metadata.get('deduplication_rate', 0)
        dedup_ok = dedup_rate < 5.0
        print(f"{'âœ…' if dedup_ok else 'âš ï¸ '} å»é‡ç‡: {dedup_rate:.2f}% (å»ºè®®: < 5%)")
        
        # 4. æ•°æ®å®Œæ•´æ€§
        complete_ok = all([
            'sentences' in data,
            'vocab' in data,
            'metadata' in data
        ])
        print(f"{'âœ…' if complete_ok else 'âŒ'} æ•°æ®å®Œæ•´æ€§: {'å®Œæ•´' if complete_ok else 'ç¼ºå¤±å­—æ®µ'}")
        
        # æ€»ç»“
        print("\nã€éªŒè¯ç»“æœã€‘")
        all_ok = sentence_ok and vocab_ok and length_ok and dedup_ok and complete_ok
        
        if all_ok:
            print("ğŸ‰ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼æ•°æ®é›†è´¨é‡åˆæ ¼ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
            return True
        else:
            print("âš ï¸  éƒ¨åˆ†æ£€æŸ¥æœªé€šè¿‡ï¼Œå»ºè®®:")
            if not sentence_ok:
                print("  - é‡æ–°è¿è¡Œ fetch_large_wikipedia.py ç»§ç»­é‡‡é›†")
            if not vocab_ok:
                print("  - å¢åŠ é‡‡é›†è½®æ•°ä»¥æ‰©å……è¯æ±‡é‡")
            if not length_ok:
                print("  - æ•°æ®å¯ç”¨ï¼Œä½†å¥é•¿åˆ†å¸ƒç•¥æœ‰åå·®")
            if not dedup_ok:
                print("  - å»é‡ç‡ç•¥é«˜ï¼Œä½†ä»åœ¨å¯æ¥å—èŒƒå›´")
            return False
            
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ {file_path} ä¸å­˜åœ¨")
        print("  è¯·å…ˆè¿è¡Œ: python fetch_large_wikipedia.py")
        return False
    except json.JSONDecodeError:
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ {file_path} æ ¼å¼é”™è¯¯")
        return False
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        return False

if __name__ == '__main__':
    success = verify_dataset()
    sys.exit(0 if success else 1)
