"""
å¿«é€Ÿå¼€å§‹ç¤ºä¾‹ | Quick Start Example
=================================

è¿™ä¸ªè„šæœ¬æ¼”ç¤ºå¦‚ä½•å¿«é€ŸåŠ è½½å’Œä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚
This script demonstrates how to quickly load and use the trained model.
"""

import torch
from transformers import BertTokenizer, BertForMaskedLM
import json

def load_model(model_path='stage4_large_100k_final.pth'):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ | Load the trained model"""
    print(f"ğŸ“¥ åŠ è½½æ¨¡å‹ | Loading model from {model_path}...")
    
    # åˆå§‹åŒ–tokenizer
    tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
    
    # åˆå§‹åŒ–æ¨¡å‹
    model = BertForMaskedLM.from_pretrained('bert-base-chinese')
    
    # åŠ è½½æƒé‡
    checkpoint = torch.load(model_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼| Model loaded successfully!")
    return model, tokenizer

def predict_masked_word(model, tokenizer, text):
    """é¢„æµ‹maskä½ç½®çš„è¯ | Predict the masked word"""
    print(f"\nğŸ” è¾“å…¥ | Input: {text}")
    
    # Tokenize
    inputs = tokenizer(text, return_tensors='pt')
    
    # é¢„æµ‹
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = outputs.logits
    
    # è·å–maskä½ç½®
    mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]
    
    if len(mask_token_index) == 0:
        print("âŒ æœªæ‰¾åˆ°[MASK]æ ‡è®° | No [MASK] token found")
        return None
    
    # è·å–top-5é¢„æµ‹
    mask_token_logits = predictions[0, mask_token_index, :]
    top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()
    
    print("ğŸ“Š Top 5 é¢„æµ‹ | Predictions:")
    for i, token_id in enumerate(top_5_tokens, 1):
        token = tokenizer.decode([token_id])
        print(f"   {i}. {token}")
    
    return tokenizer.decode([top_5_tokens[0]])

def main():
    """ä¸»å‡½æ•° | Main function"""
    print("=" * 60)
    print("Chinese BERT 100K - å¿«é€Ÿå¼€å§‹ | Quick Start")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_model()
    
    # ç¤ºä¾‹1ï¼šç®€å•é¢„æµ‹
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 1 | Example 1: Simple Prediction")
    print("=" * 60)
    predict_masked_word(model, tokenizer, "ä»Šå¤©å¤©æ°”å¾ˆ[MASK]ã€‚")
    
    # ç¤ºä¾‹2ï¼šæ›´å¤æ‚çš„å¥å­
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 2 | Example 2: Complex Sentence")
    print("=" * 60)
    predict_masked_word(model, tokenizer, "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœº[MASK]çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚")
    
    # ç¤ºä¾‹3ï¼šä¸“ä¸šé¢†åŸŸ
    print("\n" + "=" * 60)
    print("ç¤ºä¾‹ 3 | Example 3: Domain-Specific")
    print("=" * 60)
    predict_masked_word(model, tokenizer, "æ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒéœ€è¦å¤§é‡[MASK]ã€‚")
    
    print("\n" + "=" * 60)
    print("âœ… æ¼”ç¤ºå®Œæˆï¼| Demo completed!")
    print("=" * 60)

if __name__ == '__main__':
    main()
