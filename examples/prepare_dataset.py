"""
æ•°æ®é›†å‡†å¤‡ç¤ºä¾‹ | Dataset Preparation Example
==========================================

æ¼”ç¤ºå¦‚ä½•å‡†å¤‡å’Œå¤„ç†ä¸­æ–‡æ•°æ®é›†ç”¨äºè®­ç»ƒã€‚
Demonstrates how to prepare and process Chinese dataset for training.
"""

import json
import jieba
from collections import Counter
import random

def load_raw_data(filepath='large_wikipedia_dataset.json'):
    """åŠ è½½åŸå§‹æ•°æ®é›† | Load raw dataset"""
    print(f"ğŸ“¥ åŠ è½½åŸå§‹æ•°æ® | Loading raw data from {filepath}...")
    
    with open(filepath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    sentences = data.get('sentences', [])
    print(f"âœ… åŠ è½½å®Œæˆ | Loaded {len(sentences)} sentences")
    return sentences

def build_vocabulary(sentences, min_freq=2, max_vocab_size=10000):
    """æ„å»ºè¯æ±‡è¡¨ | Build vocabulary"""
    print(f"\nğŸ”¨ æ„å»ºè¯æ±‡è¡¨ | Building vocabulary...")
    print(f"   æœ€å°é¢‘ç‡ | Min frequency: {min_freq}")
    print(f"   æœ€å¤§è¯æ±‡é‡ | Max vocab size: {max_vocab_size}")
    
    # åˆ†è¯å¹¶ç»Ÿè®¡è¯é¢‘
    word_counts = Counter()
    for sentence in sentences:
        words = jieba.lcut(sentence)
        word_counts.update(words)
    
    # è¿‡æ»¤ä½é¢‘è¯
    vocab = {word: count for word, count in word_counts.items() 
             if count >= min_freq}
    
    # æŒ‰é¢‘ç‡æ’åºå¹¶æˆªæ–­
    sorted_vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)
    final_vocab = dict(sorted_vocab[:max_vocab_size])
    
    print(f"âœ… è¯æ±‡è¡¨æ„å»ºå®Œæˆ | Vocabulary built")
    print(f"   æ€»è¯æ•° | Total words: {len(word_counts)}")
    print(f"   è¿‡æ»¤å | After filtering: {len(vocab)}")
    print(f"   æœ€ç»ˆè¯æ±‡é‡ | Final vocab size: {len(final_vocab)}")
    
    return final_vocab

def prepare_mlm_data(sentences, mask_prob=0.15):
    """å‡†å¤‡MLMè®­ç»ƒæ•°æ® | Prepare MLM training data"""
    print(f"\nğŸ­ å‡†å¤‡MLMæ•°æ® | Preparing MLM data...")
    print(f"   é®è”½æ¦‚ç‡ | Mask probability: {mask_prob}")
    
    mlm_samples = []
    
    for sentence in sentences[:5]:  # åªå±•ç¤ºå‰5ä¸ªç¤ºä¾‹
        words = jieba.lcut(sentence)
        
        # éšæœºé®è”½
        masked_sentence = []
        original_words = []
        
        for word in words:
            if random.random() < mask_prob:
                masked_sentence.append('[MASK]')
                original_words.append(word)
            else:
                masked_sentence.append(word)
                original_words.append(word)
        
        mlm_samples.append({
            'original': ''.join(words),
            'masked': ''.join(masked_sentence),
            'targets': original_words
        })
    
    print(f"âœ… MLMæ•°æ®å‡†å¤‡å®Œæˆ | MLM data prepared")
    return mlm_samples

def print_statistics(sentences, vocab):
    """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯ | Print dataset statistics"""
    print("\n" + "=" * 60)
    print("ğŸ“Š æ•°æ®é›†ç»Ÿè®¡ | Dataset Statistics")
    print("=" * 60)
    
    # å¥å­ç»Ÿè®¡
    total_chars = sum(len(s) for s in sentences)
    avg_length = total_chars / len(sentences)
    
    print(f"å¥å­æ•°é‡ | Total sentences: {len(sentences):,}")
    print(f"æ€»å­—ç¬¦æ•° | Total characters: {total_chars:,}")
    print(f"å¹³å‡é•¿åº¦ | Average length: {avg_length:.2f} chars")
    
    # è¯æ±‡ç»Ÿè®¡
    print(f"\nè¯æ±‡é‡ | Vocabulary size: {len(vocab):,}")
    
    # Top 10é«˜é¢‘è¯
    top_words = sorted(vocab.items(), key=lambda x: x[1], reverse=True)[:10]
    print(f"\nTop 10 é«˜é¢‘è¯ | Top 10 frequent words:")
    for i, (word, count) in enumerate(top_words, 1):
        print(f"  {i:2d}. {word:8s} : {count:6,} æ¬¡")

def save_processed_data(sentences, vocab, output_prefix='processed'):
    """ä¿å­˜å¤„ç†åçš„æ•°æ® | Save processed data"""
    print(f"\nğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ® | Saving processed data...")
    
    # ä¿å­˜å¥å­
    sentences_file = f"{output_prefix}_sentences.json"
    with open(sentences_file, 'w', encoding='utf-8') as f:
        json.dump({'sentences': sentences}, f, ensure_ascii=False, indent=2)
    print(f"âœ… å¥å­å·²ä¿å­˜ | Sentences saved to {sentences_file}")
    
    # ä¿å­˜è¯æ±‡è¡¨
    vocab_file = f"{output_prefix}_vocab.json"
    with open(vocab_file, 'w', encoding='utf-8') as f:
        json.dump(vocab, f, ensure_ascii=False, indent=2)
    print(f"âœ… è¯æ±‡è¡¨å·²ä¿å­˜ | Vocabulary saved to {vocab_file}")

def main():
    """ä¸»å‡½æ•° | Main function"""
    print("=" * 60)
    print("Chinese BERT 100K - æ•°æ®é›†å‡†å¤‡ | Dataset Preparation")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    sentences = load_raw_data()
    
    # æ„å»ºè¯æ±‡è¡¨
    vocab = build_vocabulary(sentences, min_freq=2, max_vocab_size=10000)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print_statistics(sentences, vocab)
    
    # å‡†å¤‡MLMæ•°æ®ç¤ºä¾‹
    mlm_samples = prepare_mlm_data(sentences)
    
    print("\n" + "=" * 60)
    print("ğŸ“ MLMæ•°æ®ç¤ºä¾‹ | MLM Data Examples")
    print("=" * 60)
    for i, sample in enumerate(mlm_samples[:3], 1):
        print(f"\nç¤ºä¾‹ {i} | Example {i}:")
        print(f"  åŸå§‹ | Original: {sample['original']}")
        print(f"  é®è”½ | Masked: {sample['masked']}")
    
    # ä¿å­˜å¤„ç†åçš„æ•°æ®ï¼ˆå¯é€‰ï¼‰
    # save_processed_data(sentences, vocab)
    
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼| Data preparation completed!")
    print("=" * 60)

if __name__ == '__main__':
    main()
