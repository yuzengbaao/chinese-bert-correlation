# ä½¿ç”¨ç¤ºä¾‹ | Usage Examples

è¿™ä¸ªç›®å½•åŒ…å«äº†å¤šä¸ªå®ç”¨çš„ä»£ç ç¤ºä¾‹ï¼Œå¸®åŠ©ä½ å¿«é€Ÿä¸Šæ‰‹ä½¿ç”¨æœ¬é¡¹ç›®ã€‚

This directory contains practical code examples to help you get started with this project.

## ğŸ“š ç¤ºä¾‹åˆ—è¡¨ | Example List

### 1. å¿«é€Ÿå¼€å§‹ | Quick Start
**æ–‡ä»¶**: `quick_start.py`

æœ€ç®€å•çš„å…¥é—¨ç¤ºä¾‹ï¼Œå±•ç¤ºå¦‚ä½•åŠ è½½æ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹ã€‚

The simplest starter example showing how to load the model and make predictions.

```bash
python examples/quick_start.py
```

**åŠŸèƒ½ | Features:**
- åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ | Load trained model
- ä½¿ç”¨BERTè¿›è¡Œè¯è¯­é¢„æµ‹ | Use BERT for word prediction
- å±•ç¤ºTop-5é¢„æµ‹ç»“æœ | Show Top-5 predictions

---

### 2. ç»“æœåˆ†æ | Result Analysis
**æ–‡ä»¶**: `load_and_analyze.py`

æ·±å…¥åˆ†æè®­ç»ƒç»“æœï¼Œç”Ÿæˆè‡ªå®šä¹‰å¯è§†åŒ–å›¾è¡¨ã€‚

In-depth analysis of training results with custom visualizations.

```bash
python examples/load_and_analyze.py
```

**åŠŸèƒ½ | Features:**
- åŠ è½½è®­ç»ƒå†å²æ•°æ® | Load training history
- è®¡ç®—Pearsonç›¸å…³ç³»æ•°å’ŒRÂ² | Calculate Pearson r and RÂ²
- ç”Ÿæˆ4ç§è‡ªå®šä¹‰å›¾è¡¨ | Generate 4 custom plots
  * MLMå‡†ç¡®ç‡æ•£ç‚¹å›¾ | MLM accuracy scatter plot
  * æ»‘åŠ¨çª—å£å¹³å‡ | Moving average
  * æŸå¤±å‡½æ•°æ¼”å˜ | Loss function evolution
  * å‡†ç¡®ç‡åˆ†å¸ƒç›´æ–¹å›¾ | Accuracy distribution histogram

**è¾“å‡º | Output:**
- `results/custom_analysis.png` (2400x1800 åƒç´ )

---

### 3. æ•°æ®é›†å‡†å¤‡ | Dataset Preparation
**æ–‡ä»¶**: `prepare_dataset.py`

æ¼”ç¤ºå¦‚ä½•å¤„ç†ä¸­æ–‡æ–‡æœ¬æ•°æ®å¹¶å‡†å¤‡MLMè®­ç»ƒã€‚

Demonstrates how to process Chinese text data for MLM training.

```bash
python examples/prepare_dataset.py
```

**åŠŸèƒ½ | Features:**
- åŠ è½½åŸå§‹æ•°æ®é›† | Load raw dataset
- ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯ | Tokenization with jieba
- æ„å»ºè¯æ±‡è¡¨ | Build vocabulary
- å‡†å¤‡MLMè®­ç»ƒæ ·æœ¬ | Prepare MLM training samples
- æ•°æ®é›†ç»Ÿè®¡åˆ†æ | Dataset statistics

---

## ğŸš€ è¿è¡Œç¯å¢ƒ | Environment Setup

### å®‰è£…ä¾èµ– | Install Dependencies

```bash
pip install -r requirements.txt
```

### å¿…éœ€æ–‡ä»¶ | Required Files

ç¡®ä¿ä»¥ä¸‹æ–‡ä»¶å­˜åœ¨äºé¡¹ç›®æ ¹ç›®å½•ï¼š

Make sure the following files exist in the project root:

- `stage4_large_100k_final.pth` - è®­ç»ƒå¥½çš„æ¨¡å‹ | Trained model
- `training_history_100k.json` - è®­ç»ƒå†å² | Training history
- `large_wikipedia_dataset.json` - åŸå§‹æ•°æ®é›† | Raw dataset

---

## ğŸ“– ä½¿ç”¨æµç¨‹ | Usage Workflow

### æ–¹æ¡ˆAï¼šå¿«é€Ÿä½“éªŒ | Quick Experience

```bash
# 1. å¿«é€Ÿå¼€å§‹ï¼Œæµ‹è¯•æ¨¡å‹
python examples/quick_start.py

# 2. åˆ†æç»“æœ
python examples/load_and_analyze.py
```

### æ–¹æ¡ˆBï¼šå®Œæ•´æµç¨‹ | Full Workflow

```bash
# 1. å‡†å¤‡æ•°æ®é›†
python examples/prepare_dataset.py

# 2. è®­ç»ƒæ¨¡å‹ï¼ˆä½¿ç”¨ä¸»è„šæœ¬ï¼‰
python main_train.py

# 3. åˆ†æç»“æœ
python examples/load_and_analyze.py

# 4. æµ‹è¯•æ¨¡å‹
python examples/quick_start.py
```

---

## ğŸ¯ è‡ªå®šä¹‰ç¤ºä¾‹ | Custom Examples

### åˆ›å»ºä½ è‡ªå·±çš„MLMé¢„æµ‹è„šæœ¬

```python
from examples.quick_start import load_model, predict_masked_word

# åŠ è½½æ¨¡å‹
model, tokenizer = load_model()

# è‡ªå®šä¹‰å¥å­
my_sentence = "æ·±åº¦å­¦ä¹ æ˜¯[MASK]çš„é‡è¦æŠ€æœ¯ã€‚"
result = predict_masked_word(model, tokenizer, my_sentence)
print(f"é¢„æµ‹ç»“æœ: {result}")
```

### è‡ªå®šä¹‰åˆ†æè„šæœ¬

```python
from examples.load_and_analyze import load_training_history, calculate_statistics

# åŠ è½½æ•°æ®
steps, mlm_acc, loss = load_training_history()

# è®¡ç®—ä½ å…³å¿ƒçš„æŒ‡æ ‡
correlation, r_squared, slope = calculate_statistics(steps, mlm_acc)

# ä½¿ç”¨æ•°æ®è¿›è¡Œè‡ªå®šä¹‰åˆ†æ
import numpy as np
print(f"ä¸­ä½æ•°å‡†ç¡®ç‡: {np.median(mlm_acc):.2f}%")
print(f"å‡†ç¡®ç‡èŒƒå›´: {mlm_acc.max() - mlm_acc.min():.2f}%")
```

---

## ğŸ’¡ æç¤º | Tips

1. **GPUåŠ é€Ÿ**: å¦‚æœæœ‰GPUï¼Œåœ¨`quick_start.py`ä¸­ä¿®æ”¹ï¼š
   ```python
   model.to('cuda')  # ä½¿ç”¨GPU
   ```

2. **æ‰¹é‡é¢„æµ‹**: ä¿®æ”¹`quick_start.py`æ”¯æŒæ‰¹é‡å¤„ç†ï¼š
   ```python
   sentences = ["å¥å­1", "å¥å­2", "å¥å­3"]
   for sent in sentences:
       predict_masked_word(model, tokenizer, sent)
   ```

3. **ä¿å­˜ç»“æœ**: åœ¨åˆ†æè„šæœ¬ä¸­æ·»åŠ ä¿å­˜åŠŸèƒ½ï¼š
   ```python
   results = {
       'correlation': correlation,
       'r_squared': r_squared,
       'slope': slope
   }
   with open('my_analysis.json', 'w') as f:
       json.dump(results, f, indent=2)
   ```

---

## ğŸ› æ•…éšœæ’é™¤ | Troubleshooting

### é—®é¢˜1: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶

```
FileNotFoundError: [Errno 2] No such file or directory: 'stage4_large_100k_final.pth'
```

**è§£å†³**: ç¡®ä¿æ¨¡å‹æ–‡ä»¶åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œæˆ–ä¿®æ”¹è·¯å¾„ï¼š
```python
model, tokenizer = load_model('path/to/your/model.pth')
```

### é—®é¢˜2: å†…å­˜ä¸è¶³

```
RuntimeError: CUDA out of memory
```

**è§£å†³**: ä½¿ç”¨CPUåŠ è½½ï¼š
```python
checkpoint = torch.load(model_path, map_location='cpu')
```

### é—®é¢˜3: ä¸­æ–‡æ˜¾ç¤ºä¹±ç 

**è§£å†³**: å®‰è£…ä¸­æ–‡å­—ä½“ï¼š
```bash
# Windows: ç¡®ä¿å®‰è£…äº†SimHeiå­—ä½“
# Linux: sudo apt-get install fonts-wqy-zenhei
# Mac: ç³»ç»Ÿè‡ªå¸¦æ”¯æŒ
```

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ | Need Help?

- ğŸ“ æŸ¥çœ‹å®Œæ•´æ–‡æ¡£: [README.md](../README.md)
- ğŸ“Š æ•°æ®é›†è¯´æ˜: [docs/DATASET.md](../docs/DATASET.md)
- ğŸ’¬ æäº¤Issue: [GitHub Issues](https://github.com/yuzengbaao/chinese-bert-correlation/issues)

---

**Happy Coding! ç¥ç¼–ç¨‹æ„‰å¿«ï¼** ğŸ‰
