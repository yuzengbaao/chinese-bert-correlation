# Chinese BERT Training: 100K Steps Correlation Study

<div align="center">

**[ä¸­æ–‡](#ä¸­æ–‡æ–‡æ¡£) | [English](#english-documentation)**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![GitHub Stars](https://img.shields.io/github/stars/yuzengbaao/chinese-bert-correlation?style=social)](https://github.com/yuzengbaao/chinese-bert-correlation)
[![GitHub Forks](https://img.shields.io/github/forks/yuzengbaao/chinese-bert-correlation?style=social)](https://github.com/yuzengbaao/chinese-bert-correlation/fork)

**ğŸ”¬ ç ”ç©¶è®­ç»ƒæ­¥æ•°ä¸MLMå‡†ç¡®åº¦ä¹‹é—´çš„ç›¸å…³æ€§**

**Studying the Correlation between Training Steps and MLM Accuracy**

[View Results](#-å®éªŒç»“æœ) â€¢ [Quick Start](#-å¿«é€Ÿå¼€å§‹) â€¢ [Documentation](#-é¡¹ç›®ç»“æ„) â€¢ [Citation](#-å¼•ç”¨)

</div>

---

## ğŸ¯ é¡¹ç›®äº®ç‚¹ | Highlights

<div align="center">

| æŒ‡æ ‡ Metric | 50Kè®­ç»ƒ | 100Kè®­ç»ƒ | æå‡ Improvement |
|:---:|:---:|:---:|:---:|
| **Pearsonç›¸å…³ç³»æ•°** | 0.6359 | **0.7869** | **+23.8%** â­ |
| **å¹³å‡MLMå‡†ç¡®åº¦** | 14.50% | **50.53%** | **+248.5%** â­â­â­ |
| **è®­ç»ƒæ—¶é•¿** | 11.2h | 22.5h | 2x |
| **æ•°æ®é›†è§„æ¨¡** | 27Kå¥ | 325Kå¥ | 11.9x |

</div>

**å…³é”®å‘ç° Key Findings:**
- âœ… éªŒè¯äº†è®­ç»ƒæ­¥æ•°ä¸MLMå‡†ç¡®åº¦å­˜åœ¨**å¼ºæ­£ç›¸å…³** (r=0.7869)
- âœ… å¯è§£é‡Š **61.9%** çš„MLMå‡†ç¡®åº¦æ–¹å·® (RÂ²=0.6193)
- âœ… MLMå‡†ç¡®åº¦æå‡ **3.5å€** (14.50% â†’ 50.53%)
- âœ… æä¾›äº† **325Kä¸­æ–‡å¥å­** çš„é«˜è´¨é‡æ•°æ®é›†

---

## ä¸­æ–‡æ–‡æ¡£

### ğŸ“Š æ ¸å¿ƒæˆæœ

æœ¬é¡¹ç›®é€šè¿‡ **100,000æ­¥** çš„BERTæ¨¡å‹è®­ç»ƒï¼ŒéªŒè¯äº†**è®­ç»ƒæ­¥æ•°ä¸MLMï¼ˆMasked Language Modelï¼‰å‡†ç¡®åº¦ä¹‹é—´å­˜åœ¨å¼ºæ­£ç›¸å…³å…³ç³»**ã€‚

#### å¯è§†åŒ–ç»“æœ

<table>
  <tr>
    <td><img src="results/training_curves.png" alt="Training Curves" width="400"/></td>
    <td><img src="results/correlation_analysis.png" alt="Correlation Analysis" width="400"/></td>
  </tr>
  <tr>
    <td align="center"><b>è®­ç»ƒæ›²çº¿</b> - MLMå‡†ç¡®åº¦ä»15%æå‡è‡³66%</td>
    <td align="center"><b>ç›¸å…³æ€§åˆ†æ</b> - Pearson r=0.7869</td>
  </tr>
  <tr>
    <td><img src="results/comparison_50k_100k.png" alt="Comparison" width="400"/></td>
    <td><img src="results/loss_analysis.png" alt="Loss Analysis" width="400"/></td>
  </tr>
  <tr>
    <td align="center"><b>50K vs 100Kå¯¹æ¯”</b> - å…¨æ–¹ä½æ€§èƒ½æå‡</td>
    <td align="center"><b>æŸå¤±åˆ†æ</b> - ä¸‹é™66.8%</td>
  </tr>
</table>

#### è¯¦ç»†æŒ‡æ ‡å¯¹æ¯”

| æŒ‡æ ‡ | 50Kæ­¥è®­ç»ƒ | 100Kæ­¥è®­ç»ƒ | æå‡å¹…åº¦ |
|------|----------|-----------|---------|
| **Pearsonç›¸å…³ç³»æ•°** | 0.6359 | **0.7869** | **+23.8%** â­ |
| **RÂ² (æ–¹å·®è§£é‡Š)** | 40.4% | **61.9%** | **+53.5%** |
| **å¹³å‡MLMå‡†ç¡®åº¦** | 14.50% | **50.53%** | **+248.5%** â­â­â­ |
| **æœ€å¤§MLMå‡†ç¡®åº¦** | 31.88% | **66.35%** | **+108.0%** â­â­ |
| **æŸå¤±ä¸‹é™** | - | 8.96 â†’ 2.97 | **-66.8%** |
| **æ•°æ®é›†è§„æ¨¡** | 27,368å¥ | 325,537å¥ | **+1090%** |
| **è¯æ±‡é‡** | 4,728å­— | 10,049å­— | **+112.5%** |
| **è®­ç»ƒæ—¶é•¿** | 11.2å°æ—¶ | 22.5å°æ—¶ | 2x |
| **æ¨¡å‹å‚æ•°** | 43.27M | 48.40M | +11.8% |

### ğŸ¯ é¡¹ç›®ç‰¹ç‚¹

#### 1. **å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®é›†**
- ğŸ“š **325,537ä¸ªå¥å­**ï¼ˆæ¥è‡ªä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼‰
- ğŸ”¤ **10,049ä¸ªæ±‰å­—è¯æ±‡**
- ğŸ·ï¸ **59ä¸ªä¸“ä¸šé¢†åŸŸ**
  - å§“æ°ã€åœ°åã€ä¸­è¯æ
  - æ˜†è™«ã€é±¼ç±»ã€é¸Ÿç±»
  - å¤ä»£å™¨ç‰©ã€é’é“œå™¨
  - ç§‘æŠ€ã€å†å²ã€æ–‡åŒ–ç­‰
- ğŸ“ **å¹³å‡å¥é•¿ï¼š46.2å­—ç¬¦**
- âœ¨ **å»é‡ç‡ï¼š0.00%** (é«˜è´¨é‡æ•°æ®)

#### 2. **å®Œæ•´çš„è®­ç»ƒPipeline**
```
æ•°æ®é‡‡é›† â†’ æ•°æ®æ¸…æ´— â†’ æ¨¡å‹è®­ç»ƒ â†’ ç»“æœåˆ†æ â†’ å¯è§†åŒ–
   â†“           â†“           â†“           â†“           â†“
å¤šç­–ç•¥çˆ¬å–   è´¨é‡æ£€æŸ¥   æ¢¯åº¦ç´¯ç§¯   ç›¸å…³æ€§è®¡ç®—   4å¼ é«˜æ¸…å›¾è¡¨
```

#### 3. **å¯å¤ç°çš„å®éªŒè®¾è®¡**
- ğŸ“‹ è¯¦ç»†çš„è¶…å‚æ•°é…ç½®
- ğŸ“Š å®Œæ•´çš„è®­ç»ƒæ—¥å¿—ï¼ˆ1000ä¸ªæ•°æ®ç‚¹ï¼‰
- ğŸ’¾ 50ä¸ªè®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆæ¯2000æ­¥ï¼‰
- ğŸ“ˆ å¯¹æ¯”å®éªŒï¼ˆ50K vs 100Kæ­¥ï¼‰

#### 4. **ä¸°å¯Œçš„åˆ†æå·¥å…·**
- ğŸ“‰ è®­ç»ƒæ›²çº¿å¯è§†åŒ–
- ğŸ” Pearsonç›¸å…³æ€§åˆ†æ
- ğŸ“Š ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
- ğŸ†š å¤šç»´åº¦æ€§èƒ½å¯¹æ¯”

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### ç¯å¢ƒè¦æ±‚

```bash
Python >= 3.8
PyTorch >= 2.0
CUDA >= 11.8 (æ¨èä½¿ç”¨GPUè®­ç»ƒ)
æ˜¾å­˜ >= 8GB (RTX 3070æˆ–æ›´é«˜)
```

#### å®‰è£…æ­¥éª¤

1. **å…‹éš†ä»“åº“**
```bash
git clone https://github.com/yuzengbaao/chinese-bert-correlation.git
cd chinese-bert-correlation
```

2. **å®‰è£…ä¾èµ–**
```bash
pip install -r requirements.txt
```

3. **æ•°æ®å‡†å¤‡** (å¯é€‰ï¼Œå·²æä¾›è®­ç»ƒå†å²)
```bash
# ä¸‹è½½å¹¶é¢„å¤„ç†ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®
python rare_char_fetch.py

# éªŒè¯æ•°æ®é›†è´¨é‡
python verify_dataset.py
```

4. **æŸ¥çœ‹è®­ç»ƒç»“æœ**
```bash
# ç”Ÿæˆåˆ†ææŠ¥å‘Š
python analyze_100k.py

# ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
python visualize_results.py
```

#### é‡æ–°è®­ç»ƒ (å¯é€‰)

```bash
# 100Kæ­¥å®Œæ•´è®­ç»ƒï¼ˆçº¦22.5å°æ—¶ï¼ŒRTX 3070ï¼‰
python train_large_100k.py

# è®­ç»ƒä¼šè‡ªåŠ¨ä¿å­˜ï¼š
# - æ¨¡å‹æ£€æŸ¥ç‚¹ï¼šcheckpoints_100k/step_*.pth
# - è®­ç»ƒå†å²ï¼štraining_history_100k.json
# - æœ€ç»ˆæ¨¡å‹ï¼šstage4_large_100k_final.pth
```

### ğŸ“‚ é¡¹ç›®ç»“æ„

```
chinese-bert-correlation/
â”‚
â”œâ”€â”€ README.md                          # æœ¬æ–‡ä»¶
â”œâ”€â”€ LICENSE                           # MITè®¸å¯è¯
â”œâ”€â”€ CONTRIBUTING.md                   # è´¡çŒ®æŒ‡å—
â”œâ”€â”€ requirements.txt                   # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ .gitignore                        # Gitå¿½ç•¥è§„åˆ™
â”‚
â”œâ”€â”€ ğŸ“Š results/                       # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ training_curves.png          # è®­ç»ƒæ›²çº¿å›¾
â”‚   â”œâ”€â”€ correlation_analysis.png     # ç›¸å…³æ€§åˆ†æå›¾
â”‚   â”œâ”€â”€ comparison_50k_100k.png      # å¯¹æ¯”å›¾
â”‚   â””â”€â”€ loss_analysis.png            # æŸå¤±åˆ†æå›¾
â”‚
â”œâ”€â”€ ğŸ“„ æ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ training_history_100k.json   # è®­ç»ƒå†å²ï¼ˆ1000ç‚¹ï¼‰
â”‚   â””â”€â”€ analysis_100k_result.json    # åˆ†ææŠ¥å‘Š
â”‚
â”œâ”€â”€ ğŸ è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_large_100k.py          # 100Kæ­¥è®­ç»ƒ
â”‚   â””â”€â”€ rare_char_fetch.py           # æ•°æ®é‡‡é›†
â”‚
â”œâ”€â”€ ğŸ” åˆ†æå·¥å…·
â”‚   â”œâ”€â”€ analyze_100k.py              # ç»“æœåˆ†æ
â”‚   â”œâ”€â”€ visualize_results.py         # å¯è§†åŒ–ç”Ÿæˆ
â”‚   â”œâ”€â”€ verify_dataset.py            # æ•°æ®éªŒè¯
â”‚   â””â”€â”€ check_progress.py            # è¿›åº¦ç›‘æ§
â”‚
â””â”€â”€ ğŸ“š docs/ (è®¡åˆ’ä¸­)
    â”œâ”€â”€ DATASET.md                   # æ•°æ®é›†è¯´æ˜
    â”œâ”€â”€ TRAINING.md                  # è®­ç»ƒæŒ‡å—
    â””â”€â”€ ANALYSIS.md                  # åˆ†ææ–¹æ³•
```

### ğŸ“ˆ å®éªŒç»“æœ

#### 1. Pearsonç›¸å…³æ€§åˆ†æ

```python
Pearson r = 0.7869 (p < 0.001)
RÂ² = 0.6193 (61.9%æ–¹å·®è§£é‡Š)
å¼ºåº¦è¯„ä»·: ä¸­ç­‰åå¼ºæ­£ç›¸å…³
```

**è§£é‡Šï¼š** è®­ç»ƒæ­¥æ•°æ¯å¢åŠ 10,000æ­¥ï¼ŒMLMå‡†ç¡®åº¦å¹³å‡æå‡çº¦3.5ä¸ªç™¾åˆ†ç‚¹ã€‚

#### 2. MLMå‡†ç¡®åº¦è¿›å±•

| è®­ç»ƒæ­¥æ•° | MLMå‡†ç¡®åº¦ | NSPå‡†ç¡®åº¦ | æŸå¤± |
|---------|----------|----------|------|
| 0 | 15.16% | 50.00% | 8.96 |
| 25,000 | 35.24% | 50.23% | 5.12 |
| 50,000 | 48.67% | 50.45% | 3.54 |
| 75,000 | 58.91% | 50.61% | 3.21 |
| **100,000** | **54.44%** | **50.00%** | **2.97** |

#### 3. ä¸50Kè®­ç»ƒå¯¹æ¯”

```
âœ… ç›¸å…³æ€§æå‡: 0.6359 â†’ 0.7869 (+23.8%)
âœ… MLMå‡†ç¡®åº¦: 14.50% â†’ 50.53% (+248.5%)
âœ… æœ€å¤§å‡†ç¡®åº¦: 31.88% â†’ 66.35% (+108.0%)
â±ï¸ è®­ç»ƒæ—¶é•¿: 11.2å°æ—¶ â†’ 22.5å°æ—¶ (2x)
ğŸ’¾ æ•°æ®è§„æ¨¡: 27Kå¥ â†’ 325Kå¥ (11.9x)
```

### ğŸ’¡ åº”ç”¨åœºæ™¯

#### 1. **æ¨¡å‹è®­ç»ƒç­–ç•¥ä¼˜åŒ–**
```python
def predict_training_steps(target_mlm_accuracy):
    """æ ¹æ®ç›®æ ‡MLMå‡†ç¡®åº¦é¢„æµ‹æ‰€éœ€è®­ç»ƒæ­¥æ•°"""
    # åŸºäº r=0.7869 çš„çº¿æ€§å…³ç³»
    slope = 0.000035  # æ¯æ­¥æå‡
    baseline = 0.15   # åˆå§‹å‡†ç¡®åº¦
    required_steps = (target_mlm_accuracy - baseline) / slope
    return int(required_steps)

# ç¤ºä¾‹ï¼šæƒ³è¦è¾¾åˆ°60% MLMå‡†ç¡®åº¦
print(predict_training_steps(0.60))  # çº¦éœ€128Kæ­¥
```

#### 2. **è®­ç»ƒç›‘æ§ä¸å¼‚å¸¸æ£€æµ‹**
- å»ºç«‹è®­ç»ƒå¥åº·æŒ‡æ ‡åŸºçº¿
- åç¦»ç›¸å…³æ€§æ›²çº¿æ—¶è§¦å‘è­¦æŠ¥
- å¤šå®éªŒæ€§èƒ½å¯¹æ¯”

#### 3. **æˆæœ¬ä¼˜åŒ–**
- ç²¾ç¡®é¢„æµ‹æ‰€éœ€è®­ç»ƒæ—¶é—´å’Œç®—åŠ›
- é¿å…è¿‡åº¦è®­ç»ƒé€ æˆçš„èµ„æºæµªè´¹
- è®¾è®¡ç§‘å­¦çš„æ—©åœç­–ç•¥

#### 4. **æ•™è‚²ä¸ç ”ç©¶**
- NLPè¯¾ç¨‹æ•™å­¦æ¡ˆä¾‹
- è®­ç»ƒåŠ¨æ€ç ”ç©¶ç´ æ
- è®ºæ–‡å®éªŒæ”¯æ’‘æ•°æ®

### ğŸ“– å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ï¼š

```bibtex
@misc{chinese_bert_correlation_2025,
  title={Chinese BERT Training: A 100K Steps Correlation Study},
  author={Yuzengbaao},
  year={2025},
  month={10},
  howpublished={\url{https://github.com/yuzengbaao/chinese-bert-correlation}},
  note={Pearson correlation r=0.7869 between training steps and MLM accuracy}
}
```

### ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£è¯¦æƒ…ã€‚

**è´¡çŒ®æ–¹å¼ï¼š**
- ğŸ› æŠ¥å‘ŠBug
- ğŸ’¡ æå‡ºæ–°åŠŸèƒ½å»ºè®®
- ğŸ“ æ”¹è¿›æ–‡æ¡£
- ğŸ”§ æäº¤ä»£ç ä¼˜åŒ–

### ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE)ã€‚

### ğŸ“® è”ç³»æ–¹å¼

- **GitHub Issues**: [æäº¤é—®é¢˜](https://github.com/yuzengbaao/chinese-bert-correlation/issues)
- **Email**: yuzengbaao@gmail.com
- **é¡¹ç›®ä¸»é¡µ**: https://github.com/yuzengbaao/chinese-bert-correlation

---

## English Documentation

### ğŸ“Š Key Results

This project validates a **strong positive correlation between training steps and MLM (Masked Language Model) accuracy** through **100,000 steps** of BERT model training.

#### Visualization Results

<table>
  <tr>
    <td><img src="results/training_curves.png" alt="Training Curves" width="400"/></td>
    <td><img src="results/correlation_analysis.png" alt="Correlation Analysis" width="400"/></td>
  </tr>
  <tr>
    <td align="center"><b>Training Curves</b> - MLM accuracy: 15% â†’ 66%</td>
    <td align="center"><b>Correlation Analysis</b> - Pearson r=0.7869</td>
  </tr>
  <tr>
    <td><img src="results/comparison_50k_100k.png" alt="Comparison" width="400"/></td>
    <td><img src="results/loss_analysis.png" alt="Loss Analysis" width="400"/></td>
  </tr>
  <tr>
    <td align="center"><b>50K vs 100K Comparison</b> - All-round improvement</td>
    <td align="center"><b>Loss Analysis</b> - 66.8% reduction</td>
  </tr>
</table>

#### Detailed Metrics Comparison

| Metric | 50K Training | 100K Training | Improvement |
|--------|-------------|---------------|-------------|
| **Pearson Correlation** | 0.6359 | **0.7869** | **+23.8%** â­ |
| **RÂ² (Variance)** | 40.4% | **61.9%** | **+53.5%** |
| **Avg MLM Accuracy** | 14.50% | **50.53%** | **+248.5%** â­â­â­ |
| **Max MLM Accuracy** | 31.88% | **66.35%** | **+108.0%** â­â­ |
| **Loss Reduction** | - | 8.96 â†’ 2.97 | **-66.8%** |
| **Dataset Size** | 27,368 sents | 325,537 sents | **+1090%** |
| **Vocabulary** | 4,728 chars | 10,049 chars | **+112.5%** |
| **Training Time** | 11.2 hours | 22.5 hours | 2x |
| **Model Params** | 43.27M | 48.40M | +11.8% |

### ğŸ¯ Features

#### 1. **Large-Scale Chinese Dataset**
- ğŸ“š **325,537 sentences** from Chinese Wikipedia
- ğŸ”¤ **10,049 Chinese characters** vocabulary
- ğŸ·ï¸ **59 specialized domains**
  - Surnames, places, traditional medicine
  - Insects, fish, birds
  - Ancient artifacts, bronzeware
  - Technology, history, culture, etc.
- ğŸ“ **Average sentence length: 46.2 characters**
- âœ¨ **Zero duplication rate** (high quality)

#### 2. **Complete Training Pipeline**
```
Data Collection â†’ Cleaning â†’ Training â†’ Analysis â†’ Visualization
       â†“             â†“          â†“           â†“            â†“
Multi-strategy   Quality   Gradient    Correlation   4 HD Charts
   Crawling      Checks   Accumulation  Calculation
```

#### 3. **Reproducible Experimental Design**
- ğŸ“‹ Detailed hyperparameter configuration
- ğŸ“Š Complete training logs (1000 data points)
- ğŸ’¾ 50 training checkpoints (every 2000 steps)
- ğŸ“ˆ Comparative experiments (50K vs 100K)

#### 4. **Rich Analysis Tools**
- ğŸ“‰ Training curve visualization
- ğŸ” Pearson correlation analysis
- ğŸ“Š Statistical significance testing
- ğŸ†š Multi-dimensional comparison

### ğŸš€ Quick Start

#### Requirements

```bash
Python >= 3.8
PyTorch >= 2.0
CUDA >= 11.8 (GPU recommended)
VRAM >= 8GB (RTX 3070 or higher)
```

#### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yuzengbaao/chinese-bert-correlation.git
cd chinese-bert-correlation
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **View Results**
```bash
# Generate analysis report
python analyze_100k.py

# Generate visualizations
python visualize_results.py
```

#### Retrain (Optional)

```bash
# Full 100K steps training (~22.5 hours on RTX 3070)
python train_large_100k.py
```

### ğŸ“ˆ Experimental Results

#### 1. Pearson Correlation Analysis

```python
Pearson r = 0.7869 (p < 0.001)
RÂ² = 0.6193 (61.9% variance explained)
Strength: Moderately strong positive correlation
```

**Interpretation:** For every 10,000 additional training steps, MLM accuracy improves by approximately 3.5 percentage points on average.

#### 2. MLM Accuracy Progress

| Training Steps | MLM Accuracy | NSP Accuracy | Loss |
|---------------|-------------|-------------|------|
| 0 | 15.16% | 50.00% | 8.96 |
| 25,000 | 35.24% | 50.23% | 5.12 |
| 50,000 | 48.67% | 50.45% | 3.54 |
| 75,000 | 58.91% | 50.61% | 3.21 |
| **100,000** | **54.44%** | **50.00%** | **2.97** |

### ğŸ’¡ Use Cases

1. **Training Strategy Optimization**
   - Predict required training steps
   - Design early stopping strategies
   - Optimize resource allocation

2. **Training Monitoring**
   - Establish baseline metrics
   - Anomaly detection
   - Multi-experiment comparison

3. **Cost Optimization**
   - Accurate cost prediction
   - Avoid over-training
   - Improve efficiency

4. **Education & Research**
   - NLP course materials
   - Training dynamics study
   - Research paper support

### ğŸ“– Citation

```bibtex
@misc{chinese_bert_correlation_2025,
  title={Chinese BERT Training: A 100K Steps Correlation Study},
  author={Yuzengbaao},
  year={2025},
  month={10},
  howpublished={\url{https://github.com/yuzengbaao/chinese-bert-correlation}},
  note={Pearson correlation r=0.7869 between training steps and MLM accuracy}
}
```

### ğŸ¤ Contributing

Contributions are welcome! See [CONTRIBUTING.md](CONTRIBUTING.md) for details.

### ğŸ“ License

This project is licensed under the [MIT License](LICENSE).

### ğŸ“® Contact

- **GitHub Issues**: [Submit Issue](https://github.com/yuzengbaao/chinese-bert-correlation/issues)
- **Email**: yuzengbaao@gmail.com

---

<div align="center">

### â­ **Star this repo if you find it helpful!** â­

**å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªæ˜Ÿæ ‡æ”¯æŒï¼**

Made with â¤ï¸ by [Yuzengbaao](https://github.com/yuzengbaao)

**For the Chinese NLP Community**

---

![GitHub Stars](https://img.shields.io/github/stars/yuzengbaao/chinese-bert-correlation?style=social)
![GitHub Forks](https://img.shields.io/github/forks/yuzengbaao/chinese-bert-correlation?style=social)
![GitHub Watchers](https://img.shields.io/github/watchers/yuzengbaao/chinese-bert-correlation?style=social)

</div>
