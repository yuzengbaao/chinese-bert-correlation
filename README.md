# Chinese BERT Training: 100K Steps Correlation Study# AGI ç§å­ç®—æ³• - ç¬¬ä¸€æ€§åŸç†è®¾è®¡



<div align="center">## ğŸŒ± æ ¸å¿ƒå“²å­¦



**[ä¸­æ–‡](#ä¸­æ–‡æ–‡æ¡£) | [English](#english-documentation)**> "æ™ºèƒ½æ˜¯ä¸€ä¸ªç³»ç»Ÿé€šè¿‡æœ€å°åŒ–é¢„æµ‹è¯¯å·®æ¥å‹ç¼©ç»éªŒçš„èƒ½åŠ›"



[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)åŸºäºç¬¬ä¸€æ€§åŸç†ï¼Œæˆ‘ä»¬å°†æ™ºèƒ½åˆ†è§£ä¸ºä¸‰ä¸ªæœ€åŸºæœ¬çš„åŸè¯­ï¼š

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)1. **è¡¨ç¤º** (Representation) - å¦‚ä½•ç¼–ç ä¸–ç•Œ

2. **é¢„æµ‹** (Prediction) - å¦‚ä½•ç†è§£æ¨¡å¼

**ç ”ç©¶è®­ç»ƒæ­¥æ•°ä¸MLMå‡†ç¡®åº¦ä¹‹é—´çš„ç›¸å…³æ€§ | Studying the Correlation between Training Steps and MLM Accuracy**3. **é€‚åº”** (Adaptation) - å¦‚ä½•è‡ªæˆ‘æ”¹è¿›



</div>## ğŸ“ ç¬¬ä¸€æ€§åŸç†æ¨å¯¼



---### å…¬ç† 1: ä¿¡æ¯å‹ç¼©åŸç†

- æ™ºèƒ½ = ç”¨æœ€å°‘çš„ä¿¡æ¯è¡¨ç¤ºæœ€å¤šçš„ç»éªŒ

## ä¸­æ–‡æ–‡æ¡£- Kolmogorov å¤æ‚åº¦ â†’ æœ€ä¼˜å‹ç¼© = ç†è§£



### ğŸ“Š æ ¸å¿ƒæˆæœ### å…¬ç† 2: é¢„æµ‹æœ€å°åŒ–åŸç†

- å­¦ä¹  = å‡å°‘æœªæ¥çš„æ„å¤–

æœ¬é¡¹ç›®é€šè¿‡ **100,000æ­¥** çš„BERTæ¨¡å‹è®­ç»ƒï¼ŒéªŒè¯äº†**è®­ç»ƒæ­¥æ•°ä¸MLMï¼ˆMasked Language Modelï¼‰å‡†ç¡®åº¦ä¹‹é—´å­˜åœ¨å¼ºæ­£ç›¸å…³å…³ç³»**ï¼š- è‡ªç”±èƒ½åŸç† (Free Energy Principle)



| æŒ‡æ ‡ | 50Kæ­¥è®­ç»ƒ | 100Kæ­¥è®­ç»ƒ | æå‡å¹…åº¦ |### å…¬ç† 3: é€’å½’è‡ªä¸¾åŸç†

|------|----------|-----------|---------|- ç®€å•è§„åˆ™ Ã— å¤§è§„æ¨¡è¿­ä»£ = å¤æ‚æ¶Œç°

| **Pearsonç›¸å…³ç³»æ•°** | 0.6359 | **0.7869** | **+23.8%** â­ |- è‡ªæˆ‘æŒ‡æ¶‰ â†’ å…ƒå­¦ä¹ èƒ½åŠ›

| **å¹³å‡MLMå‡†ç¡®åº¦** | 14.50% | **50.53%** | **+248.5%** â­â­â­ |

| **æœ€å¤§MLMå‡†ç¡®åº¦** | 31.88% | **66.35%** | **+108.0%** |## ğŸ§¬ ç§å­ç®—æ³•æ¶æ„

| **æ•°æ®é›†è§„æ¨¡** | 27,368å¥ | 325,537å¥ | **11.9x** |

| **è¯æ±‡é‡** | 4,728å­— | 10,049å­— | **2.1x** |```

è¾“å…¥åºåˆ— â†’ [çŠ¶æ€ç©ºé—´] â†’ é¢„æµ‹è¾“å‡º

**å…³é”®å‘ç°**ï¼š              â†‘  â†“

- âœ… Pearsonç›¸å…³ç³»æ•° **r = 0.7869**ï¼ˆå¼ºæ­£ç›¸å…³ï¼‰           è¯¯å·®åé¦ˆ â†’ çŠ¶æ€æ›´æ–°

- âœ… å¯è§£é‡Š **61.9%** çš„MLMå‡†ç¡®åº¦æ–¹å·®              â†“

- âœ… MLMå‡†ç¡®åº¦æå‡ **3.5å€**         æ¶Œç°å¤æ‚è¡Œä¸º

- âœ… æŸå¤±å‡½æ•°ä¸‹é™ **66.8%**ï¼ˆ8.96 â†’ 2.97ï¼‰```



### ğŸ¯ é¡¹ç›®ç‰¹ç‚¹## ğŸ¯ è®¾è®¡åŸåˆ™



1. **å¤§è§„æ¨¡ä¸­æ–‡æ•°æ®é›†**1. **æœ€å°æ€§**: ä¸å¯å†ç®€åŒ–

   - 325,537ä¸ªå¥å­ï¼ˆæ¥è‡ªä¸­æ–‡ç»´åŸºç™¾ç§‘ï¼‰2. **é€šç”¨æ€§**: é€‚ç”¨äºä»»ä½•åŸŸ

   - 10,049ä¸ªæ±‰å­—è¯æ±‡3. **å¯æ‰©å±•**: è§„æ¨¡å®šå¾‹æ”¯æŒ

   - 59ä¸ªä¸“ä¸šé¢†åŸŸï¼ˆå§“æ°ã€åœ°åã€ä¸­è¯æã€æ˜†è™«ã€å¤ä»£å™¨ç‰©ç­‰ï¼‰4. **è‡ªç»„ç»‡**: æ— éœ€äººå·¥è®¾è®¡ç‰¹å¾

   - å¹³å‡å¥é•¿ï¼š46.2å­—ç¬¦5. **å¯å¤ç°**: å®Œå…¨ç¡®å®šæ€§ï¼ˆç»™å®šéšæœºç§å­ï¼‰



2. **å®Œæ•´çš„è®­ç»ƒPipeline**## ğŸ“Š é¢„æœŸæ¶Œç°èƒ½åŠ›

   - æ•°æ®é‡‡é›†ï¼šå¤šç­–ç•¥Wikipediaçˆ¬å–

   - æ•°æ®æ¸…æ´—ï¼šå»é‡ã€è´¨é‡æ£€æŸ¥å½“è§„æ¨¡æ‰©å¤§æ—¶ï¼Œå°†æ¶Œç°ï¼š

   - æ¨¡å‹è®­ç»ƒï¼šæ¢¯åº¦ç´¯ç§¯ã€å­¦ä¹ ç‡è°ƒåº¦- æŠ½è±¡è¡¨ç¤º

   - ç»“æœåˆ†æï¼šç›¸å…³æ€§è®¡ç®—ã€å¯è§†åŒ–- ç»„åˆæ³›åŒ–

- å…ƒå­¦ä¹ 

3. **å¯å¤ç°çš„å®éªŒè®¾è®¡**- å› æœæ¨ç†

   - è¯¦ç»†çš„è¶…å‚æ•°é…ç½®- ...ï¼ˆæ›´å¤šé«˜é˜¶èƒ½åŠ›ï¼‰

   - å®Œæ•´çš„è®­ç»ƒæ—¥å¿—ï¼ˆ1000ä¸ªæ•°æ®ç‚¹ï¼‰

   - 50ä¸ªè®­ç»ƒæ£€æŸ¥ç‚¹ï¼ˆæ¯2000æ­¥ï¼‰## ğŸ”¬ å®éªŒéªŒè¯è·¯å¾„

   - å¯¹æ¯”å®éªŒï¼ˆ50K vs 100Kæ­¥ï¼‰

1. ç©å…·é—®é¢˜ï¼ˆåºåˆ—é¢„æµ‹ï¼‰

4. **ä¸°å¯Œçš„åˆ†æå·¥å…·**2. ç®€å•ç¯å¢ƒï¼ˆç½‘æ ¼ä¸–ç•Œï¼‰

   - è®­ç»ƒæ›²çº¿å¯è§†åŒ–3. è¯­è¨€å»ºæ¨¡

   - ç›¸å…³æ€§åˆ†ææŠ¥å‘Š4. å¤šæ¨¡æ€ç†è§£

   - ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ5. å¼€æ”¾åŸŸæ¨ç†

   - å¤šç»´åº¦æ€§èƒ½å¯¹æ¯”

---

### ğŸš€ å¿«é€Ÿå¼€å§‹

**å…³é”®æ´å¯Ÿ**: ä¸è¦è®¾è®¡"æ™ºèƒ½"ï¼Œè€Œæ˜¯è®¾è®¡"å¯ä»¥å­¦ä¹ æ™ºèƒ½çš„ç³»ç»Ÿ"

#### ç¯å¢ƒè¦æ±‚

```bash
Python >= 3.8
PyTorch >= 2.0
CUDA >= 11.8 (æ¨èä½¿ç”¨GPUè®­ç»ƒ)
```

#### å®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

#### æ•°æ®å‡†å¤‡

```bash
# 1. ä¸‹è½½å¹¶é¢„å¤„ç†ä¸­æ–‡ç»´åŸºç™¾ç§‘æ•°æ®
python rare_char_fetch.py

# 2. éªŒè¯æ•°æ®é›†è´¨é‡
python verify_dataset.py
```

#### å¼€å§‹è®­ç»ƒ

```bash
# 100Kæ­¥å®Œæ•´è®­ç»ƒï¼ˆçº¦22.5å°æ—¶ï¼ŒRTX 3070ï¼‰
python train_large_100k.py

# è®­ç»ƒä¼šè‡ªåŠ¨ä¿å­˜ï¼š
# - æ¨¡å‹æ£€æŸ¥ç‚¹ï¼šcheckpoints_100k/step_*.pth
# - è®­ç»ƒå†å²ï¼štraining_history_100k.json
# - æœ€ç»ˆæ¨¡å‹ï¼šstage4_large_100k_final.pth
```

#### ç»“æœåˆ†æ

```bash
# ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š
python analyze_100k.py

# è¾“å‡ºæ–‡ä»¶ï¼š
# - analysis_100k_result.jsonï¼ˆæ•°å€¼ç»“æœï¼‰
# - æ§åˆ¶å°è¾“å‡ºï¼ˆè¯¦ç»†ç»Ÿè®¡ï¼‰
```

### ğŸ“‚ é¡¹ç›®ç»“æ„

```
AGI/
â”œâ”€â”€ README.md                          # æœ¬æ–‡ä»¶
â”œâ”€â”€ requirements.txt                   # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ LICENSE                           # MITè®¸å¯è¯
â”‚
â”œâ”€â”€ data/                             # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ large_wikipedia_dataset.json  # 325Kå¥å­æ•°æ®é›†
â”‚   â””â”€â”€ vocab.txt                     # 10Kè¯æ±‡è¡¨
â”‚
â”œâ”€â”€ scripts/                          # æ•°æ®é‡‡é›†è„šæœ¬
â”‚   â”œâ”€â”€ rare_char_fetch.py           # ç¨€æœ‰å­—ç¬¦é‡‡é›†
â”‚   â”œâ”€â”€ verify_dataset.py            # æ•°æ®é›†éªŒè¯
â”‚   â””â”€â”€ check_progress.py            # è¿›åº¦æ£€æŸ¥
â”‚
â”œâ”€â”€ training/                         # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_large_100k.py          # 100Kæ­¥è®­ç»ƒ
â”‚   â””â”€â”€ model.py                     # BERTæ¨¡å‹å®šä¹‰
â”‚
â”œâ”€â”€ analysis/                         # åˆ†æå·¥å…·
â”‚   â”œâ”€â”€ analyze_100k.py              # ç»“æœåˆ†æ
â”‚   â””â”€â”€ visualize.py                 # å¯è§†åŒ–ç”Ÿæˆ
â”‚
â”œâ”€â”€ results/                          # å®éªŒç»“æœ
â”‚   â”œâ”€â”€ stage4_large_100k_final.pth  # æœ€ç»ˆæ¨¡å‹ï¼ˆ193MBï¼‰
â”‚   â”œâ”€â”€ training_history_100k.json   # è®­ç»ƒå†å²ï¼ˆ1000ç‚¹ï¼‰
â”‚   â”œâ”€â”€ analysis_100k_result.json    # åˆ†ææŠ¥å‘Š
â”‚   â””â”€â”€ checkpoints_100k/            # 50ä¸ªæ£€æŸ¥ç‚¹
â”‚
â””â”€â”€ docs/                            # æ–‡æ¡£
    â”œâ”€â”€ DATASET.md                   # æ•°æ®é›†è¯´æ˜
    â”œâ”€â”€ TRAINING.md                  # è®­ç»ƒæŒ‡å—
    â””â”€â”€ ANALYSIS.md                  # åˆ†ææ–¹æ³•
```

### ğŸ“ˆ å®éªŒç»“æœ

#### 1. Pearsonç›¸å…³æ€§åˆ†æ

```
Pearson r = 0.7869 (p < 0.001)
RÂ² = 0.6193 (61.9%æ–¹å·®è§£é‡Š)
å¼ºåº¦è¯„ä»·: ä¸­ç­‰åå¼ºæ­£ç›¸å…³
```

**è§£é‡Š**ï¼šè®­ç»ƒæ­¥æ•°æ¯å¢åŠ 10,000æ­¥ï¼ŒMLMå‡†ç¡®åº¦å¹³å‡æå‡çº¦3.5ä¸ªç™¾åˆ†ç‚¹ã€‚

#### 2. MLMå‡†ç¡®åº¦è¿›å±•

| è®­ç»ƒæ­¥æ•° | MLMå‡†ç¡®åº¦ | æŸå¤± |
|---------|----------|------|
| 0 | 15.16% | 8.96 |
| 25,000 | 35.24% | 5.12 |
| 50,000 | 48.67% | 3.54 |
| 75,000 | 58.91% | 3.21 |
| 100,000 | 54.44% | 2.97 |

#### 3. ä¸50Kè®­ç»ƒå¯¹æ¯”

```
ç›¸å…³æ€§æå‡: 0.6359 â†’ 0.7869 (+23.8%)
MLMå‡†ç¡®åº¦: 14.50% â†’ 50.53% (+248.5%)
è®­ç»ƒæ—¶é•¿: 11.2å°æ—¶ â†’ 22.5å°æ—¶
```

### ğŸ’¡ åº”ç”¨åœºæ™¯

1. **æ¨¡å‹è®­ç»ƒç­–ç•¥ä¼˜åŒ–**
   - æ ¹æ®ç›®æ ‡æ€§èƒ½é¢„æµ‹æ‰€éœ€è®­ç»ƒæ­¥æ•°
   - è®¾è®¡ç§‘å­¦çš„æ—©åœç­–ç•¥
   - ä¼˜åŒ–èµ„æºåˆ†é…

2. **LLMè®­ç»ƒç›‘æ§**
   - å»ºç«‹è®­ç»ƒå¥åº·æŒ‡æ ‡
   - å¼‚å¸¸æ£€æµ‹ï¼ˆåç¦»ç›¸å…³æ€§æ›²çº¿ï¼‰
   - å¤šå®éªŒå¯¹æ¯”åŸºçº¿

3. **æ•™è‚²ä¸ç ”ç©¶**
   - NLPè¯¾ç¨‹æ•™å­¦æ¡ˆä¾‹
   - è®ºæ–‡å®éªŒæ”¯æ’‘
   - å¼€æºç¤¾åŒºè´¡çŒ®

4. **æˆæœ¬ä¼˜åŒ–**
   - ç²¾ç¡®é¢„æµ‹è®­ç»ƒæˆæœ¬
   - é¿å…è¿‡åº¦è®­ç»ƒ
   - æé«˜è®­ç»ƒæ•ˆç‡

### ğŸ“– å¼•ç”¨

å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨çš„ç ”ç©¶æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ï¼š

```bibtex
@misc{chinese_bert_correlation_2025,
  title={Chinese BERT Training: A 100K Steps Correlation Study},
  author={Your Name},
  year={2025},
  howpublished={\url{https://github.com/yourusername/chinese-bert-correlation}},
  note={Studying the correlation between training steps and MLM accuracy using 325K Chinese sentences}
}
```

### ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ï¼è¯·æŸ¥çœ‹ [CONTRIBUTING.md](CONTRIBUTING.md) äº†è§£è¯¦æƒ…ã€‚

### ğŸ“ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE)ã€‚

### ğŸ“® è”ç³»æ–¹å¼

- Issue: [GitHub Issues](https://github.com/yourusername/chinese-bert-correlation/issues)
- Email: your.email@example.com

---

## English Documentation

### ğŸ“Š Key Results

This project validates a **strong positive correlation between training steps and MLM (Masked Language Model) accuracy** through **100,000 steps** of BERT model training:

| Metric | 50K Training | 100K Training | Improvement |
|--------|-------------|---------------|-------------|
| **Pearson Correlation** | 0.6359 | **0.7869** | **+23.8%** â­ |
| **Avg MLM Accuracy** | 14.50% | **50.53%** | **+248.5%** â­â­â­ |
| **Max MLM Accuracy** | 31.88% | **66.35%** | **+108.0%** |
| **Dataset Size** | 27,368 sents | 325,537 sents | **11.9x** |
| **Vocabulary** | 4,728 chars | 10,049 chars | **2.1x** |

**Key Findings**:
- âœ… Pearson correlation coefficient **r = 0.7869** (strong positive)
- âœ… Explains **61.9%** of MLM accuracy variance
- âœ… MLM accuracy improved by **3.5x**
- âœ… Loss decreased by **66.8%** (8.96 â†’ 2.97)

### ğŸ¯ Features

1. **Large-Scale Chinese Dataset**
   - 325,537 sentences (from Chinese Wikipedia)
   - 10,049 Chinese character vocabulary
   - 59 specialized domains (surnames, places, TCM, insects, artifacts, etc.)
   - Average sentence length: 46.2 characters

2. **Complete Training Pipeline**
   - Data collection: Multi-strategy Wikipedia crawling
   - Data cleaning: Deduplication, quality checks
   - Model training: Gradient accumulation, learning rate scheduling
   - Result analysis: Correlation calculation, visualization

3. **Reproducible Experimental Design**
   - Detailed hyperparameter configuration
   - Complete training logs (1000 data points)
   - 50 training checkpoints (every 2000 steps)
   - Comparative experiments (50K vs 100K steps)

4. **Rich Analysis Tools**
   - Training curve visualization
   - Correlation analysis reports
   - Statistical significance testing
   - Multi-dimensional performance comparison

### ğŸš€ Quick Start

#### Requirements

```bash
Python >= 3.8
PyTorch >= 2.0
CUDA >= 11.8 (GPU training recommended)
```

#### Install Dependencies

```bash
pip install -r requirements.txt
```

#### Data Preparation

```bash
# 1. Download and preprocess Chinese Wikipedia data
python rare_char_fetch.py

# 2. Verify dataset quality
python verify_dataset.py
```

#### Start Training

```bash
# Full 100K steps training (~22.5 hours on RTX 3070)
python train_large_100k.py

# Auto-saves:
# - Model checkpoints: checkpoints_100k/step_*.pth
# - Training history: training_history_100k.json
# - Final model: stage4_large_100k_final.pth
```

#### Result Analysis

```bash
# Generate comprehensive analysis report
python analyze_100k.py

# Output files:
# - analysis_100k_result.json (numerical results)
# - Console output (detailed statistics)
```

### ğŸ“ˆ Experimental Results

#### 1. Pearson Correlation Analysis

```
Pearson r = 0.7869 (p < 0.001)
RÂ² = 0.6193 (61.9% variance explained)
Strength: Moderately strong positive correlation
```

**Interpretation**: For every 10,000 additional training steps, MLM accuracy improves by approximately 3.5 percentage points on average.

#### 2. MLM Accuracy Progress

| Training Steps | MLM Accuracy | Loss |
|---------------|-------------|------|
| 0 | 15.16% | 8.96 |
| 25,000 | 35.24% | 5.12 |
| 50,000 | 48.67% | 3.54 |
| 75,000 | 58.91% | 3.21 |
| 100,000 | 54.44% | 2.97 |

#### 3. Comparison with 50K Training

```
Correlation improvement: 0.6359 â†’ 0.7869 (+23.8%)
MLM accuracy: 14.50% â†’ 50.53% (+248.5%)
Training time: 11.2 hours â†’ 22.5 hours
```

### ğŸ’¡ Use Cases

1. **Model Training Strategy Optimization**
   - Predict required training steps based on target performance
   - Design scientific early stopping strategies
   - Optimize resource allocation

2. **LLM Training Monitoring**
   - Establish training health metrics
   - Anomaly detection (deviation from correlation curve)
   - Multi-experiment comparison baseline

3. **Education & Research**
   - NLP course teaching cases
   - Research paper experimental support
   - Open-source community contributions

4. **Cost Optimization**
   - Accurately predict training costs
   - Avoid over-training
   - Improve training efficiency

### ğŸ“– Citation

If this project helps your research, please cite:

```bibtex
@misc{chinese_bert_correlation_2025,
  title={Chinese BERT Training: A 100K Steps Correlation Study},
  author={Your Name},
  year={2025},
  howpublished={\url{https://github.com/yourusername/chinese-bert-correlation}},
  note={Studying the correlation between training steps and MLM accuracy using 325K Chinese sentences}
}
```

### ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.

### ğŸ“ License

This project is licensed under the [MIT License](LICENSE).

### ğŸ“® Contact

- Issues: [GitHub Issues](https://github.com/yourusername/chinese-bert-correlation/issues)
- Email: your.email@example.com

---

<div align="center">

**â­ If you find this project helpful, please give it a star! â­**

**å¦‚æœæœ¬é¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªæ˜Ÿæ ‡æ”¯æŒï¼**

Made with â¤ï¸ for the Chinese NLP Community

</div>
